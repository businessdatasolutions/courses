```{r}
library(ggplot2)
library(readr)
library(dplyr)
library(tidyr)
```


# Business Problems and Data Science Solutions  {#businessproblems}


## Connecting business to data

In the following video you'll hear an expert's thoughts on the relation between business problems and data challenges. What is this relationship? What questions could be asked during data understanding?

<iframe width="560" height="315" src="https://www.youtube.com/embed/xo0bsiiQ9cM" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

Big Data is not reserved for Big Business. In the following video you'll see a case of a small retailer using data to improve his business decisions. What were the business questions the company had? How did they use data to support their decisions?  

<iframe width="560" height="315" src="https://www.youtube.com/embed/hYoRMqkN_TI" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

## Supervised, Unsupervised and Reinforcement Methods

### Supervised vs Unsupervised Methods

<iframe width="560" height="315" src="https://www.youtube.com/embed/1XG1b85zPUk" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

### Reinforcement Methods
<iframe width="560" height="315" src="https://www.youtube.com/embed/kopoLzvh5jY" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

## The Data Mining Process

```{r datamining-fig, echo=FALSE, fig.align='center', fig.asp=.75, fig.cap='CRISP-DM Model taken from: https://commons.wikimedia.org/wiki/File:CRISP-DM_Process_Diagram.png', message=TRUE, warning=TRUE, out.width='80%'}
knitr::include_graphics(rep('images/crisp.png'))
```

### Formulating a business problem

**Business Case:** Recognizing hand-written digits

```{r echo=FALSE, message=TRUE, warning=TRUE}
mnemonic <- c("E", "C", "L", "I", "P", "SE")
question <- c("Expectation: Why does the user want the information?", "Client Group: For whom is the service / product intended?", "Location:  Where is the service / product sited?", "Impact: What would represent success?  How is this measured?", "Professionals: Who provides or improves the service / product?", "Service: What type of service / product is under consideration?")
answer <- c("A utility company wants to scan the electricity meter readings it receives from clients by paper forms and put it into a database", "Electricity consumers", "This is an internal process", "An accuracy of 99%", "At the moment this is done by hand", "The readings are used to prepare invoices")

headers <- c("Mnemonic", "Question", "Answer")
programmeDF <- data.frame(mnemonic, question, answer)
colnames(programmeDF) <- headers

knitr::kable(
  programmeDF, longtable = TRUE, booktabs = TRUE,
  caption = 'ECLIPSE questions taken from @wildridge_eclipse_2002'
)
```


### Data Understanding

We are going to train a model on data from the MNIST database [@lecun1998mnist]. The original dataset contains 60000 images of handwritten digits. The size of each images is 28x28 pixel and each pixel has a grayscale value between 0 (white) en 255 (black).

```{r echo=FALSE, fig.cap="Example taken from @robinson_exploring_2018", message=FALSE, warning=FALSE}
mnist_raw <- read_csv("datasets/mnist.csv")

pixels_gathered <- mnist_raw %>%
  rename(label = X1) %>%
  mutate(instance = row_number()) %>%
  gather(pixel, value, -label, -instance) %>%
  tidyr::extract(pixel, "pixel", "(\\d+)", convert = TRUE) %>%
  mutate(pixel = pixel - 2, x = pixel %% 28, y = 28 - pixel %/% 28)

theme_set(theme_light())
pixels_gathered %>%
  filter(instance <= 12) %>%
  ggplot(aes(x, y, fill = value)) +
  geom_tile() +
  facet_wrap(~ instance + label)
```

### Data Preparation

*How much gray is there in the set of images?*

Example Mnist @robinson_exploring_2018
```{r echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Example taken from @robinson_exploring_2018"}
ggplot(pixels_gathered, aes(value)) +
  geom_histogram()
```

<h3>Solution</h3>
  <p>Before checking, analyze the chart and draw your own conclusions.</p>
  <button type="button" class="btn btn-info" data-toggle="collapse" data-target="#grayscale">Solution</button>
  <div id="grayscale" class="collapse">
  <hr>
Most pixels in the dataset are completely white, along with another set of pixels that are completely dark, with relatively few in between.

**Think**: Let's assume the images were black and whit photographs of landscapes or faces. Would you expect the same color distribution as above?

</div>
<hr>

*How much variability there is within each digit label. Do all 3s look like each other, and what is the “most typical” example of a 6?*

To answer this, we can find the mean value for each position within each label and visualize these average digits as ten separate facets. These averaged images are called centroids. We’re treating each image as a 784-dimensional point (28 by 28), and then taking the average of all points in each dimension individually.

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Example taken from @robinson_exploring_2018"}
pixel_summary <- pixels_gathered %>%
  group_by(x, y, label) %>%
  summarize(mean_value = mean(value)) %>%
  ungroup()

pixel_summary %>%
  ggplot(aes(x, y, fill = mean_value)) +
  geom_tile() +
  scale_fill_gradient2(low = "white", high = "black", mid = "gray", midpoint = 127.5) +
  facet_wrap(~ label, nrow = 2) +
  labs(title = "Average value of each pixel in 10 MNIST digits", fill = "Average value") +
  theme_void()
```

<h3>Solution</h3>
  <p>Before checking, analyze the chart and draw your own conclusions.</p>
  <button type="button" class="btn btn-info" data-toggle="collapse" data-target="#variability">Solution</button>
  <div id="variability" class="collapse">
  <hr>
Already we have some suspicions about which digits might be easier to separate. Distinguishing 0 and 1 looks pretty straightforward: you could pick a few pixels at the center (always dark in 1 but not 0), or at the left and right edges (often dark in 0 but not 1), and you’d have a pretty great classifier. Pairs like 4/9, or 3/8, have a lot more overlap and will be a more challenging problem.
</div>
<hr>

*Which digits have more variability on average?*

So far, this machine learning problem might seem a bit easy: we have some very “typical” versions of each digit. But one of the reasons classification can be challenging is that some digits will fall widely outside the norm. It’s useful to explore atypical cases, since it could help us understand why the method fails and help us choose a method and engineer features.

In this case, we could consider the Euclidean distance (square root of the sum of squares) of each image to its label’s centroid.

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Example taken from @robinson_exploring_2018"}
pixels_joined <- pixels_gathered %>%
  inner_join(pixel_summary, by = c("label", "x", "y"))
image_distances <- pixels_joined %>%
  group_by(label, instance) %>%
  summarize(euclidean_distance = sqrt(mean((value - mean_value) ^ 2)))

ggplot(image_distances, aes(factor(label), euclidean_distance)) +
  geom_boxplot() +
  labs(x = "Digit", y = "Euclidean distance to the digit centroid")
```
<h3>Solution</h3>
  <p>Before checking, analyze the chart and draw your own conclusions.</p>
  <button type="button" class="btn btn-info" data-toggle="collapse" data-target="#atypical">Solution</button>
  <div id="atypical" class="collapse">
  <hr>
  It looks like 1s have especially low distances to their centroid: for the most part there’s not a ton of variability in how people draw that digit. It looks like the most variability by this measure are in 0s and 2s. But every digit has at least a few cases with an unusually large distance from their centroid. I wonder what those look like?

To discover this, we can visualize the six digit instances that had the least resemblance to their central digit.

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Example taken from @robinson_exploring_2018"}
worst_instances <- image_distances %>%
  top_n(6, euclidean_distance) %>%
  mutate(number = rank(-euclidean_distance))
pixels_gathered %>%
  inner_join(worst_instances, by = c("label", "instance")) %>%
  ggplot(aes(x, y, fill = value)) +
  geom_tile(show.legend = FALSE) +
  scale_fill_gradient2(low = "white", high = "black", mid = "gray", midpoint = 127.5) +
  facet_grid(label ~ number) +
  labs(title = "Least typical digits", subtitle = "The 6 digits within each label that had the greatest distance to the centroid") +
  theme_void() +
  theme(strip.text = element_blank())
```
This is a useful way to understand what kinds of problems the data could have. For instance, while most 1s looked the same, they could be drawn diagonally, or with a flat line and a flag on top. A 7 could be drawn with a bar in the middle. (And what is up with that 9 on the lower left?)

This also gives us a realistic sense of how accurate our classifier can get. Even humans would have a hard time classifying some of these sloppy digits, so we can’t expect a 100% success rate. (Conversely, if one of our classifiers does get a 100% success rate, we should examine whether we’re overfitting!).
</div>
<hr>

*How easy it is to tell pairs of digits apart?8

To examine this, we could try overlapping pairs of our centroid digits, and taking the difference between them. If two centroids have very little overlap, this means they’ll probably be easy to distinguish.

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Example taken from @robinson_exploring_2018"}
digit_differences <- crossing(compare1 = 0:9, compare2 = 0:9) %>%
  filter(compare1 != compare2) %>%
  mutate(negative = compare1, positive = compare2) %>%
  gather(class, label, positive, negative) %>%
  inner_join(pixel_summary, by = "label") %>%
  select(-label) %>%
  spread(class, mean_value)
ggplot(digit_differences, aes(x, y, fill = positive - negative)) +
  geom_tile() +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = .5) +
  facet_grid(compare2 ~ compare1) +
  theme_void() +
  labs(title = "Pixels that distinguish pairs of MNIST images", subtitle = "Red means the pixel is darker for that row's digit, and blue means the pixel is darker for that column's digit.")
```

<h3>Solution</h3>
  <p>Before checking, analyze the chart and draw your own conclusions.</p>
  <button type="button" class="btn btn-info" data-toggle="collapse" data-target="#atypical">Solution</button>
  <div id="atypical" class="collapse">
  <hr>
  Pairs with very red or very blue regions will be easy to classify, since they describe features that divide the datasets neatly. This confirms our suspicion about 0/1 being easy to classify: it has substantial regions than are deeply red or blue.

Comparisons that are largely white may be more difficult. We can see 4/9 looks pretty challenging, which makes sense (a handwritten 4 and 9 really differ only by a small region at the top). 7/9 shows a similar challenge.
  </div>
  <hr>

### Modeling
Example Mnist @robinson_exploring_2018

### Evaluation and Deployment
Example Mnist @robinson_exploring_2018

[MNIST Handwriting](https://codepen.io/arguiot/pen/xPYRKZ)

<iframe height="600" style="width: 100%;" scrolling="no" title="MNIST handwritten digits - ML" src="https://codepen.io/arguiot/embed/xPYRKZ?height=600&theme-id=light&default-tab=result" frameborder="no" loading="lazy" allowtransparency="true" allowfullscreen="true">
  See the Pen <a href='https://codepen.io/arguiot/pen/xPYRKZ'>MNIST handwritten digits - ML</a> by Arthur  Guiot
  (<a href='https://codepen.io/arguiot'>@arguiot</a>) on <a href='https://codepen.io'>CodePen</a>.
</iframe>

## Data Science vs Software Development

## Other Data Science Skills and Tooling

### Math and Statistics

### Databases

#### SQL

#### NoSQL

#### Graph

