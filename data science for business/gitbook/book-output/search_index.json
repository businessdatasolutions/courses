[["businessproblems.html", "Module 2 Business Problems and Data Science Solutions 2.1 Connecting business to data 2.2 Supervised, Unsupervised and Reinforcement Methods 2.3 The Data Mining Process 2.4 Data Science vs Software Development 2.5 Other Data Science Skills and Tooling", " Module 2 Business Problems and Data Science Solutions 2.1 Connecting business to data In the following video you’ll hear an expert’s thoughts on the relation between business problems and data challenges. What is this relationship? What questions could be asked during data understanding? Big Data is not reserved for Big Business. In the following video you’ll see a case of a small retailer using data to improve his business decisions. What were the business questions the company had? How did they use data to support their decisions? 2.2 Supervised, Unsupervised and Reinforcement Methods 2.2.1 Supervised vs Unsupervised Methods 2.2.2 Reinforcement Methods 2.3 The Data Mining Process Figure 2.1: CRISP-DM Model taken from: https://commons.wikimedia.org/wiki/File:CRISP-DM_Process_Diagram.png 2.3.1 Formulating a business problem Business Case: Recognizing hand-written digits Table 2.1: ECLIPSE questions taken from Wildridge and Bell (2002) Mnemonic Question Answer E Expectation: Why does the user want the information? A utility company wants to scan the electricity meter readings it receives from clients by paper forms and put it into a database C Client Group: For whom is the service / product intended? Electricity consumers L Location: Where is the service / product sited? This is an internal process I Impact: What would represent success? How is this measured? An accuracy of 99% P Professionals: Who provides or improves the service / product? At the moment this is done by hand SE Service: What type of service / product is under consideration? The readings are used to prepare invoices 2.3.2 Data Understanding We are going to train a model on data from the MNIST database (LeCun 1998). The original dataset contains 60000 images of handwritten digits. The size of each images is 28x28 pixel and each pixel has a grayscale value between 0 (white) en 255 (black). Figure 2.2: Example taken from Robinson (2018) 2.3.3 Data Preparation How much gray is there in the set of images? Example Mnist Robinson (2018) Figure 2.3: Example taken from Robinson (2018) Solution Before checking, analyze the chart and draw your own conclusions. Solution Most pixels in the dataset are completely white, along with another set of pixels that are completely dark, with relatively few in between. Think: Let’s assume the images were black and whit photographs of landscapes or faces. Would you expect the same color distribution as above? How much variability there is within each digit label. Do all 3s look like each other, and what is the “most typical” example of a 6? To answer this, we can find the mean value for each position within each label and visualize these average digits as ten separate facets. These averaged images are called centroids. We’re treating each image as a 784-dimensional point (28 by 28), and then taking the average of all points in each dimension individually. Figure 2.4: Example taken from Robinson (2018) Solution Before checking, analyze the chart and draw your own conclusions. Solution Already we have some suspicions about which digits might be easier to separate. Distinguishing 0 and 1 looks pretty straightforward: you could pick a few pixels at the center (always dark in 1 but not 0), or at the left and right edges (often dark in 0 but not 1), and you’d have a pretty great classifier. Pairs like 4/9, or 3/8, have a lot more overlap and will be a more challenging problem. Which digits have more variability on average? So far, this machine learning problem might seem a bit easy: we have some very “typical” versions of each digit. But one of the reasons classification can be challenging is that some digits will fall widely outside the norm. It’s useful to explore atypical cases, since it could help us understand why the method fails and help us choose a method and engineer features. In this case, we could consider the Euclidean distance (square root of the sum of squares) of each image to its label’s centroid. Figure 2.5: Example taken from Robinson (2018) Solution Before checking, analyze the chart and draw your own conclusions. Solution It looks like 1s have especially low distances to their centroid: for the most part there’s not a ton of variability in how people draw that digit. It looks like the most variability by this measure are in 0s and 2s. But every digit has at least a few cases with an unusually large distance from their centroid. I wonder what those look like? To discover this, we can visualize the six digit instances that had the least resemblance to their central digit. Figure 2.6: Example taken from Robinson (2018) This is a useful way to understand what kinds of problems the data could have. For instance, while most 1s looked the same, they could be drawn diagonally, or with a flat line and a flag on top. A 7 could be drawn with a bar in the middle. (And what is up with that 9 on the lower left?) This also gives us a realistic sense of how accurate our classifier can get. Even humans would have a hard time classifying some of these sloppy digits, so we can’t expect a 100% success rate. (Conversely, if one of our classifiers does get a 100% success rate, we should examine whether we’re overfitting!). *How easy it is to tell pairs of digits apart?8 To examine this, we could try overlapping pairs of our centroid digits, and taking the difference between them. If two centroids have very little overlap, this means they’ll probably be easy to distinguish. Figure 2.7: Example taken from Robinson (2018) Solution Before checking, analyze the chart and draw your own conclusions. Solution Pairs with very red or very blue regions will be easy to classify, since they describe features that divide the datasets neatly. This confirms our suspicion about 0/1 being easy to classify: it has substantial regions than are deeply red or blue. Comparisons that are largely white may be more difficult. We can see 4/9 looks pretty challenging, which makes sense (a handwritten 4 and 9 really differ only by a small region at the top). 7/9 shows a similar challenge. 2.3.4 Modeling Example Mnist Robinson (2018) 2.3.5 Evaluation and Deployment Example Mnist Robinson (2018) MNIST Handwriting See the Pen MNIST handwritten digits - ML by Arthur Guiot ((???)) on CodePen. 2.4 Data Science vs Software Development 2.5 Other Data Science Skills and Tooling 2.5.1 Math and Statistics 2.5.2 Databases 2.5.2.1 SQL 2.5.2.2 NoSQL 2.5.2.3 Graph References "]]
