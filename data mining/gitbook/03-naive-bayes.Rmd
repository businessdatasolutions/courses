# Probabilistic Learning with Naive Bayes Classification {#naivebayes}

<iframe width="560" height="315" src="https://www.youtube.com/embed/O2L2Uv9pdDA" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

```{r message=FALSE, warning=FALSE, include=FALSE}
library(tidyverse)
library(tm)
library(caret)
```

## Business Case: Filtering Spam
In 2020 spam account for more than 50% of total e-mail traffic [@noauthor_spam_nodate]. This illustrates the value of a good spam filter. Naive Bayes spam filtering is a standard technique for handling spam. It is one of the oldest ways of doing spam filtering, with roots in the 1990s.

## Data Understanding
The data we'll use comes from the SMS Spam Collection [@noauthor_uci_spam_nodate]. It contains a set SMS messages that are label 'ham' or 'spam'. and is a standard data set for testing spam filtering methods.

```{r}
url <- "datasets/smsspam.csv"
rawDF <- read_csv(url)
head(rawDF)
```

The dataset has `r dim(rawDF)[2]` variables (columns) and `r dim(rawDF)[1]` observations (rows).

The variable `type` is of class `character`. As it indicates whether the message belongs to the category ham or spam we should convert it to a factor variable.

```{r}
rawDF$type <- rawDF$type %>% factor
class(rawDF$type)
```


## Preparation
First we need to create a corpus, which refers to a collection of text documents. In our case each sms is considered a text document. We'll use the `Corpus()` function from the`tm` package.

```{r}
rawCorpus <- Corpus(VectorSource(rawDF$text))
inspect(rawCorpus[1:3])
```

The corpus contains `r length(rawCorpus)` documents. Which obviously matches with the number of rows in our dataset.

We will use the function `tm_map()` to do some first cleaning up. First we'll change everything to lowercase. We'll also remove numbers as these will contain litle information on a message being spam or not.

```{r}
cleanCorpus <- rawCorpus %>% tm_map(tolower) %>% tm_map(removeNumbers)
```

For computation efficiency it is important to eliminate all items from a dataset of which you're rather confident that they' do'll add little information to your model. In our case we can expect that words like "and" or "but" will be equally common in both ham and spam messages. We should therefore filter them out before we start modeling. We'll also remove punctuation.

```{r}
cleanCorpus <- cleanCorpus %>% tm_map(tolower) %>% tm_map(removeWords, stopwords()) %>% tm_map(removePunctuation)
```
Now that we have removed certain items, the text lines contain a lot of whitespaces where these items used to be. In our last step we will remove additional whitespace.

```{r}
cleanCorpus <- cleanCorpus %>% tm_map(stripWhitespace)
```
Let's inspect the corpus again. Compare it to the raw version.

```{r}
tibble(Raw = rawCorpus$content[1:3], Clean = cleanCorpus$content[1:3])
```
Now that we have cleaned up the texts, we are going to transform the messages to a matrix. Each word in the each message will get its own column, each row will be a message and the cells of the matrix will contain a word count.

```{r}
cleanDtm <- cleanCorpus %>% DocumentTermMatrix
inspect(cleanDtm)
```


## Modeling



## Evaluation


**Questions:** 


