<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Module 3 Probabilistic Learning with Naive Bayes Classification | Data Mining</title>
  <meta name="description" content="This is a compagnon to the course Data Mining." />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Module 3 Probabilistic Learning with Naive Bayes Classification | Data Mining" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a compagnon to the course Data Mining." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Module 3 Probabilistic Learning with Naive Bayes Classification | Data Mining" />
  
  <meta name="twitter:description" content="This is a compagnon to the course Data Mining." />
  

<meta name="author" content="Witek ten hove" />


<meta name="date" content="2021-01-13" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="knn.html"/>
<link rel="next" href="references.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Data Science for Business</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#prerequisites"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#purpose-of-this-course"><i class="fa fa-check"></i>Purpose of this course</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#structure-of-the-course"><i class="fa fa-check"></i>Structure of the course</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="dm.html"><a href="dm.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="dm.html"><a href="dm.html#working-with-rmarkdown"><i class="fa fa-check"></i><b>1.1</b> Working with RMarkdown</a></li>
<li class="chapter" data-level="1.2" data-path="dm.html"><a href="dm.html#working-with-git-and-github"><i class="fa fa-check"></i><b>1.2</b> Working with Git and Github</a></li>
<li class="chapter" data-level="1.3" data-path="dm.html"><a href="dm.html#customizing-rmarkdown-with-html-and-css"><i class="fa fa-check"></i><b>1.3</b> Customizing RMarkdown with HTML and CSS</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="knn.html"><a href="knn.html"><i class="fa fa-check"></i><b>2</b> Lazy learning with k-Nearest Neighbors</a><ul>
<li class="chapter" data-level="2.1" data-path="knn.html"><a href="knn.html#business-case-diagnosing-breast-cancer"><i class="fa fa-check"></i><b>2.1</b> Business Case: Diagnosing Breast Cancer</a></li>
<li class="chapter" data-level="2.2" data-path="knn.html"><a href="knn.html#data-understanding"><i class="fa fa-check"></i><b>2.2</b> Data Understanding</a></li>
<li class="chapter" data-level="2.3" data-path="knn.html"><a href="knn.html#preparation"><i class="fa fa-check"></i><b>2.3</b> Preparation</a></li>
<li class="chapter" data-level="2.4" data-path="knn.html"><a href="knn.html#modeling"><i class="fa fa-check"></i><b>2.4</b> Modeling</a></li>
<li class="chapter" data-level="2.5" data-path="knn.html"><a href="knn.html#evaluation"><i class="fa fa-check"></i><b>2.5</b> Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="naivebayes.html"><a href="naivebayes.html"><i class="fa fa-check"></i><b>3</b> Probabilistic Learning with Naive Bayes Classification</a><ul>
<li class="chapter" data-level="3.1" data-path="naivebayes.html"><a href="naivebayes.html#business-case-filtering-spam"><i class="fa fa-check"></i><b>3.1</b> Business Case: Filtering Spam</a></li>
<li class="chapter" data-level="3.2" data-path="naivebayes.html"><a href="naivebayes.html#data-understanding-1"><i class="fa fa-check"></i><b>3.2</b> Data Understanding</a></li>
<li class="chapter" data-level="3.3" data-path="naivebayes.html"><a href="naivebayes.html#preparation-1"><i class="fa fa-check"></i><b>3.3</b> Preparation</a></li>
<li class="chapter" data-level="3.4" data-path="naivebayes.html"><a href="naivebayes.html#modeling-1"><i class="fa fa-check"></i><b>3.4</b> Modeling</a></li>
<li class="chapter" data-level="3.5" data-path="naivebayes.html"><a href="naivebayes.html#evaluation-1"><i class="fa fa-check"></i><b>3.5</b> Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Data Mining</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="naivebayes" class="section level1">
<h1><span class="header-section-number">Module 3</span> Probabilistic Learning with Naive Bayes Classification</h1>
<iframe width="560" height="315" src="https://www.youtube.com/embed/O2L2Uv9pdDA" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
<div id="business-case-filtering-spam" class="section level2">
<h2><span class="header-section-number">3.1</span> Business Case: Filtering Spam</h2>
<p>In 2020 spam account for more than 50% of total e-mail traffic <span class="citation">(“Spam Statistics: Spam E-Mail Traffic Share 2019” <a href="#ref-noauthor_spam_nodate" role="doc-biblioref">n.d.</a>)</span>. This illustrates the value of a good spam filter. Naive Bayes spam filtering is a standard technique for handling spam. It is one of the oldest ways of doing spam filtering, with roots in the 1990s.</p>
</div>
<div id="data-understanding-1" class="section level2">
<h2><span class="header-section-number">3.2</span> Data Understanding</h2>
<p>The data we’ll use comes from the SMS Spam Collection <span class="citation">(“UCI Machine Learning Repository: SMS Spam Collection Data Set” <a href="#ref-noauthor_uci_spam_nodate" role="doc-biblioref">n.d.</a>)</span>. It contains a set SMS messages that are label ‘ham’ or ‘spam’. and is a standard data set for testing spam filtering methods.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="naivebayes.html#cb1-1"></a>url &lt;-<span class="st"> &quot;datasets/smsspam.csv&quot;</span></span>
<span id="cb1-2"><a href="naivebayes.html#cb1-2"></a>rawDF &lt;-<span class="st"> </span><span class="kw">read_csv</span>(url)</span></code></pre></div>
<pre><code>## 
## ── Column specification ────────────────────────────────────────────────────────
## cols(
##   type = col_character(),
##   text = col_character()
## )</code></pre>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="naivebayes.html#cb3-1"></a><span class="kw">head</span>(rawDF)</span></code></pre></div>
<pre><code>## # A tibble: 6 x 2
##   type  text                                                                    
##   &lt;chr&gt; &lt;chr&gt;                                                                   
## 1 ham   Go until jurong point, crazy.. Available only in bugis n great world la…
## 2 ham   Ok lar... Joking wif u oni...                                           
## 3 spam  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Tex…
## 4 ham   U dun say so early hor... U c already then say...                       
## 5 ham   Nah I don&#39;t think he goes to usf, he lives around here though           
## 6 spam  FreeMsg Hey there darling it&#39;s been 3 week&#39;s now and no word back! I&#39;d …</code></pre>
<p>The dataset has 2 variables (columns) and 5572 observations (rows).</p>
<p>The variable <code>type</code> is of class <code>character</code>. As it indicates whether the message belongs to the category ham or spam we should convert it to a factor variable.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="naivebayes.html#cb5-1"></a>rawDF<span class="op">$</span>type &lt;-<span class="st"> </span>rawDF<span class="op">$</span>type <span class="op">%&gt;%</span><span class="st"> </span>factor <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">relevel</span>(<span class="st">&quot;spam&quot;</span>)</span>
<span id="cb5-2"><a href="naivebayes.html#cb5-2"></a><span class="kw">class</span>(rawDF<span class="op">$</span>type)</span></code></pre></div>
<pre><code>## [1] &quot;factor&quot;</code></pre>
<p>We can also visually inspect the data by creating wordclouds for each sms type.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="naivebayes.html#cb7-1"></a>spam &lt;-<span class="st"> </span>rawDF <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(type <span class="op">==</span><span class="st"> &quot;spam&quot;</span>)</span>
<span id="cb7-2"><a href="naivebayes.html#cb7-2"></a>ham &lt;-<span class="st"> </span>rawDF <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(type <span class="op">==</span><span class="st"> &quot;ham&quot;</span>)</span>
<span id="cb7-3"><a href="naivebayes.html#cb7-3"></a></span>
<span id="cb7-4"><a href="naivebayes.html#cb7-4"></a><span class="kw">wordcloud</span>(spam<span class="op">$</span>text, <span class="dt">max.words =</span> <span class="dv">20</span>, <span class="dt">scale =</span> <span class="kw">c</span>(<span class="dv">4</span>, <span class="fl">0.8</span>), <span class="dt">colors=</span> <span class="kw">c</span>(<span class="st">&quot;indianred1&quot;</span>,<span class="st">&quot;indianred2&quot;</span>,<span class="st">&quot;indianred3&quot;</span>,<span class="st">&quot;indianred&quot;</span>))</span>
<span id="cb7-5"><a href="naivebayes.html#cb7-5"></a><span class="kw">wordcloud</span>(ham<span class="op">$</span>text, <span class="dt">max.words =</span> <span class="dv">20</span>, <span class="dt">scale =</span> <span class="kw">c</span>(<span class="dv">4</span>, <span class="fl">0.8</span>), <span class="dt">colors=</span> <span class="kw">c</span>(<span class="st">&quot;lightsteelblue1&quot;</span>,<span class="st">&quot;lightsteelblue2&quot;</span>,<span class="st">&quot;lightsteelblue3&quot;</span>,<span class="st">&quot;lightsteelblue&quot;</span>))</span></code></pre></div>
<p><img src="gitbook_files/figure-html/figures-side-1.png" width="50%" /><img src="gitbook_files/figure-html/figures-side-2.png" width="50%" />
Question:</p>
<ul>
<li><em>What differences do you notice?</em></li>
</ul>
</div>
<div id="preparation-1" class="section level2">
<h2><span class="header-section-number">3.3</span> Preparation</h2>
<p>First we need to create a corpus, which refers to a collection of text documents. In our case each sms is considered a text document. We’ll use the <code>Corpus()</code> function from the<code>tm</code> package.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="naivebayes.html#cb8-1"></a>rawCorpus &lt;-<span class="st"> </span><span class="kw">Corpus</span>(<span class="kw">VectorSource</span>(rawDF<span class="op">$</span>text))</span>
<span id="cb8-2"><a href="naivebayes.html#cb8-2"></a><span class="kw">inspect</span>(rawCorpus[<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>])</span></code></pre></div>
<pre><code>## &lt;&lt;SimpleCorpus&gt;&gt;
## Metadata:  corpus specific: 1, document level (indexed): 0
## Content:  documents: 3
## 
## [1] Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...                                            
## [2] Ok lar... Joking wif u oni...                                                                                                                              
## [3] Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C&#39;s apply 08452810075over18&#39;s</code></pre>
<p>The corpus contains 5572 documents. Which obviously matches with the number of rows in our dataset.</p>
<p>We will use the function <code>tm_map()</code> to do some first cleaning up. First we’ll change everything to lowercase. We’ll also remove numbers as these will contain litle information on a message being spam or not.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="naivebayes.html#cb10-1"></a>cleanCorpus &lt;-<span class="st"> </span>rawCorpus <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">tm_map</span>(tolower) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">tm_map</span>(removeNumbers)</span></code></pre></div>
<pre><code>## Warning in tm_map.SimpleCorpus(., tolower): transformation drops documents</code></pre>
<pre><code>## Warning in tm_map.SimpleCorpus(., removeNumbers): transformation drops documents</code></pre>
<p>For computation efficiency it is important to eliminate all items from a dataset of which you’re rather confident that they’ do’ll add little information to your model. In our case we can expect that words like “and” or “but” will be equally common in both ham and spam messages. We should therefore filter them out before we start modeling. We’ll also remove punctuation.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="naivebayes.html#cb13-1"></a>cleanCorpus &lt;-<span class="st"> </span>cleanCorpus <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">tm_map</span>(tolower) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">tm_map</span>(removeWords, <span class="kw">stopwords</span>()) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">tm_map</span>(removePunctuation)</span></code></pre></div>
<pre><code>## Warning in tm_map.SimpleCorpus(., tolower): transformation drops documents</code></pre>
<pre><code>## Warning in tm_map.SimpleCorpus(., removeWords, stopwords()): transformation
## drops documents</code></pre>
<pre><code>## Warning in tm_map.SimpleCorpus(., removePunctuation): transformation drops
## documents</code></pre>
<p>Now that we have removed certain items, the text lines contain a lot of whitespaces where these items used to be. In our last step we will remove additional whitespace.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="naivebayes.html#cb17-1"></a>cleanCorpus &lt;-<span class="st"> </span>cleanCorpus <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">tm_map</span>(stripWhitespace)</span></code></pre></div>
<pre><code>## Warning in tm_map.SimpleCorpus(., stripWhitespace): transformation drops
## documents</code></pre>
<p>Let’s inspect the corpus again. Compare it to the raw version.</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="naivebayes.html#cb19-1"></a><span class="kw">tibble</span>(<span class="dt">Raw =</span> rawCorpus<span class="op">$</span>content[<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>], <span class="dt">Clean =</span> cleanCorpus<span class="op">$</span>content[<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>])</span></code></pre></div>
<pre><code>## # A tibble: 3 x 2
##   Raw                                     Clean                                 
##   &lt;chr&gt;                                   &lt;chr&gt;                                 
## 1 Go until jurong point, crazy.. Availab… go jurong point crazy available bugis…
## 2 Ok lar... Joking wif u oni...           ok lar joking wif u oni               
## 3 Free entry in 2 a wkly comp to win FA … free entry wkly comp win fa cup final…</code></pre>
<p>Now that we have cleaned up the texts, we are going to transform the messages to a matrix. Each word in the each message will get its own column, each row will be a message and the cells of the matrix will contain a word count.</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="naivebayes.html#cb21-1"></a>cleanDTM &lt;-<span class="st"> </span>cleanCorpus <span class="op">%&gt;%</span><span class="st"> </span>DocumentTermMatrix</span>
<span id="cb21-2"><a href="naivebayes.html#cb21-2"></a><span class="kw">inspect</span>(cleanDTM)</span></code></pre></div>
<pre><code>## &lt;&lt;DocumentTermMatrix (documents: 5572, terms: 7829)&gt;&gt;
## Non-/sparse entries: 42822/43580366
## Sparsity           : 100%
## Maximal term length: 40
## Weighting          : term frequency (tf)
## Sample             :
##       Terms
## Docs   call can free get just know like ltgt now will
##   1085    0   0    0   1    0    0    1    0   0   11
##   1579    0   0    0   0    0    0    0   18   0    0
##   1863    0   0    0   0    0    1    1    0   0    0
##   2158    0   0    0   0    0    0    0    0   0    0
##   2370    0   0    1   0    0    0    1    0   0    0
##   2380    0   1    0   0    0    0    0    1   0    0
##   2434    0   3    1   1    0    0    0    6   0    0
##   2848    0   0    0   0    0    0    0    0   0    0
##   3016    0   0    0   0    0    0    0    2   0    0
##   5105    0   0    1   0    0    0    1    0   0    0</code></pre>
<p>Before we start modeling we need to split all datasets into train and test sets. For this we will use a function from the <code>caret</code> package. The function <code>createDataPartition()</code> can be used to create balanced splits of the data. If the y argument to this function is a factor, the random sampling occurs within each class and should preserve the overall class distribution of the data. In this case we’ll create a 75/25% split.</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="naivebayes.html#cb23-1"></a><span class="co"># Create split indices</span></span>
<span id="cb23-2"><a href="naivebayes.html#cb23-2"></a><span class="kw">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb23-3"><a href="naivebayes.html#cb23-3"></a>trainIndex &lt;-<span class="st"> </span><span class="kw">createDataPartition</span>(rawDF<span class="op">$</span>type, <span class="dt">p =</span> <span class="fl">.75</span>, </span>
<span id="cb23-4"><a href="naivebayes.html#cb23-4"></a>                                  <span class="dt">list =</span> <span class="ot">FALSE</span>, </span>
<span id="cb23-5"><a href="naivebayes.html#cb23-5"></a>                                  <span class="dt">times =</span> <span class="dv">1</span>)</span>
<span id="cb23-6"><a href="naivebayes.html#cb23-6"></a><span class="kw">head</span>(trainIndex)</span></code></pre></div>
<pre><code>##      Resample1
## [1,]         3
## [2,]         4
## [3,]         5
## [4,]         6
## [5,]         7
## [6,]        10</code></pre>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="naivebayes.html#cb25-1"></a><span class="co"># Apply split indices to DF</span></span>
<span id="cb25-2"><a href="naivebayes.html#cb25-2"></a>trainDF &lt;-<span class="st"> </span>rawDF[trainIndex, ]</span></code></pre></div>
<pre><code>## Warning: The `i` argument of ``[`()` can&#39;t be a matrix as of tibble 3.0.0.
## Convert to a vector.
## This warning is displayed once every 8 hours.
## Call `lifecycle::last_warnings()` to see where this warning was generated.</code></pre>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="naivebayes.html#cb27-1"></a>testDF &lt;-<span class="st"> </span>rawDF[<span class="op">-</span>trainIndex, ]</span>
<span id="cb27-2"><a href="naivebayes.html#cb27-2"></a></span>
<span id="cb27-3"><a href="naivebayes.html#cb27-3"></a><span class="co"># Apply split indices to Corpus</span></span>
<span id="cb27-4"><a href="naivebayes.html#cb27-4"></a>trainCorpus &lt;-<span class="st"> </span>cleanCorpus[trainIndex]</span>
<span id="cb27-5"><a href="naivebayes.html#cb27-5"></a>testCorpus &lt;-<span class="st"> </span>cleanCorpus[<span class="op">-</span>trainIndex]</span>
<span id="cb27-6"><a href="naivebayes.html#cb27-6"></a></span>
<span id="cb27-7"><a href="naivebayes.html#cb27-7"></a><span class="co"># Apply split indices to DTM</span></span>
<span id="cb27-8"><a href="naivebayes.html#cb27-8"></a>trainDTM &lt;-<span class="st"> </span>cleanDTM[trainIndex, ]</span>
<span id="cb27-9"><a href="naivebayes.html#cb27-9"></a>testDTM &lt;-<span class="st"> </span>cleanDTM[<span class="op">-</span>trainIndex, ]</span></code></pre></div>
<p>As you can check (how?) the DTM has almost 7800 features. Remember that each feature in the DTM is a word. Some words will have very few counts and therefore will have limited predictive power. To save on computation time we will eliminate words with low frequencies.</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb28-1"><a href="naivebayes.html#cb28-1"></a>freqWords &lt;-<span class="st"> </span>trainDTM <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">findFreqTerms</span>(<span class="dv">5</span>)</span>
<span id="cb28-2"><a href="naivebayes.html#cb28-2"></a>trainDTM &lt;-<span class="st">  </span><span class="kw">DocumentTermMatrix</span>(trainCorpus, <span class="kw">list</span>(<span class="dt">dictionary =</span> freqWords))</span>
<span id="cb28-3"><a href="naivebayes.html#cb28-3"></a>testDTM &lt;-<span class="st">  </span><span class="kw">DocumentTermMatrix</span>(testCorpus, <span class="kw">list</span>(<span class="dt">dictionary =</span> freqWords))</span></code></pre></div>
<p>With this operation we’ve reduced the number of features to around 1200.</p>
<p>Another issue is that the Naive Bayes classifier is typically trained on categorical features. We now have numerical matrix with word counts. We will transform the counts into a factor that simply indicates whether the word appears in the document or not. We’ll first build our own function for this and then apply it to each column in the DTM.</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="naivebayes.html#cb29-1"></a>convert_counts &lt;-<span class="st"> </span><span class="cf">function</span>(x) {</span>
<span id="cb29-2"><a href="naivebayes.html#cb29-2"></a>  x &lt;-<span class="st"> </span><span class="kw">ifelse</span>(x <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">factor</span>(<span class="dt">levels =</span> <span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>), <span class="dt">labels =</span> <span class="kw">c</span>(<span class="st">&quot;No&quot;</span>, <span class="st">&quot;Yes&quot;</span>))</span>
<span id="cb29-3"><a href="naivebayes.html#cb29-3"></a>}</span>
<span id="cb29-4"><a href="naivebayes.html#cb29-4"></a></span>
<span id="cb29-5"><a href="naivebayes.html#cb29-5"></a>nColsDTM &lt;-<span class="st"> </span><span class="kw">dim</span>(trainDTM)[<span class="dv">2</span>]</span>
<span id="cb29-6"><a href="naivebayes.html#cb29-6"></a>trainDTM &lt;-<span class="st"> </span><span class="kw">apply</span>(trainDTM, <span class="dt">MARGIN =</span> <span class="dv">2</span>, convert_counts)</span>
<span id="cb29-7"><a href="naivebayes.html#cb29-7"></a>testDTM &lt;-<span class="st"> </span><span class="kw">apply</span>(testDTM, <span class="dt">MARGIN =</span> <span class="dv">2</span>, convert_counts)</span>
<span id="cb29-8"><a href="naivebayes.html#cb29-8"></a></span>
<span id="cb29-9"><a href="naivebayes.html#cb29-9"></a><span class="kw">head</span>(trainDTM[,<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>])</span></code></pre></div>
<pre><code>##     Terms
## Docs apply comp  cup   entry final free  may   receive text  txt  
##    1 &quot;Yes&quot; &quot;Yes&quot; &quot;Yes&quot; &quot;Yes&quot; &quot;Yes&quot; &quot;Yes&quot; &quot;Yes&quot; &quot;Yes&quot;   &quot;Yes&quot; &quot;Yes&quot;
##    2 &quot;No&quot;  &quot;No&quot;  &quot;No&quot;  &quot;No&quot;  &quot;No&quot;  &quot;No&quot;  &quot;No&quot;  &quot;No&quot;    &quot;No&quot;  &quot;No&quot; 
##    3 &quot;No&quot;  &quot;No&quot;  &quot;No&quot;  &quot;No&quot;  &quot;No&quot;  &quot;No&quot;  &quot;No&quot;  &quot;No&quot;    &quot;No&quot;  &quot;No&quot; 
##    4 &quot;No&quot;  &quot;No&quot;  &quot;No&quot;  &quot;No&quot;  &quot;No&quot;  &quot;No&quot;  &quot;No&quot;  &quot;No&quot;    &quot;No&quot;  &quot;No&quot; 
##    5 &quot;No&quot;  &quot;No&quot;  &quot;No&quot;  &quot;No&quot;  &quot;No&quot;  &quot;No&quot;  &quot;No&quot;  &quot;No&quot;    &quot;No&quot;  &quot;No&quot; 
##    6 &quot;No&quot;  &quot;No&quot;  &quot;No&quot;  &quot;No&quot;  &quot;No&quot;  &quot;Yes&quot; &quot;No&quot;  &quot;No&quot;    &quot;No&quot;  &quot;No&quot;</code></pre>
</div>
<div id="modeling-1" class="section level2">
<h2><span class="header-section-number">3.4</span> Modeling</h2>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="naivebayes.html#cb31-1"></a>nbayesModel &lt;-<span class="st">  </span><span class="kw">naiveBayes</span>(trainDTM, trainDF<span class="op">$</span>type, <span class="dt">laplace =</span> <span class="dv">1</span>)</span>
<span id="cb31-2"><a href="naivebayes.html#cb31-2"></a>predVec &lt;-<span class="st"> </span><span class="kw">predict</span>(nbayesModel, testDTM)</span></code></pre></div>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="naivebayes.html#cb32-1"></a><span class="kw">confusionMatrix</span>(predVec, testDF<span class="op">$</span>type, <span class="dt">positive =</span> <span class="st">&quot;spam&quot;</span>, <span class="dt">dnn =</span> <span class="kw">c</span>(<span class="st">&quot;Prediction&quot;</span>, <span class="st">&quot;True&quot;</span>))</span></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           True
## Prediction spam  ham
##       spam  173   26
##       ham    13 1180
##                                         
##                Accuracy : 0.972         
##                  95% CI : (0.9619, 0.98)
##     No Information Rate : 0.8664        
##     P-Value [Acc &gt; NIR] : &lt; 2e-16       
##                                         
##                   Kappa : 0.8825        
##                                         
##  Mcnemar&#39;s Test P-Value : 0.05466       
##                                         
##             Sensitivity : 0.9301        
##             Specificity : 0.9784        
##          Pos Pred Value : 0.8693        
##          Neg Pred Value : 0.9891        
##              Prevalence : 0.1336        
##          Detection Rate : 0.1243        
##    Detection Prevalence : 0.1430        
##       Balanced Accuracy : 0.9543        
##                                         
##        &#39;Positive&#39; Class : spam          
## </code></pre>
</div>
<div id="evaluation-1" class="section level2">
<h2><span class="header-section-number">3.5</span> Evaluation</h2>
<div class="figure" style="text-align: center"><span id="fig:difftable1-fig"></span>
<img src="images/diffusion.png" alt="Standard diffusion table. Taken from: https://emj.bmj.com/content/emermed/36/7/431/F1.large.jpg" width="80%" />
<p class="caption">
Figure 3.1: Standard diffusion table. Taken from: <a href="https://emj.bmj.com/content/emermed/36/7/431/F1.large.jpg" class="uri">https://emj.bmj.com/content/emermed/36/7/431/F1.large.jpg</a>
</p>
</div>
<p><strong>Questions:</strong></p>
<ol style="list-style-type: decimal">
<li><em>How would you assess the overall performance of the model?</em></li>
<li><em>What would you consider as more costly: high false negatives or high false positives levels? Why?</em></li>
</ol>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-noauthor_spam_nodate">
<p>“Spam Statistics: Spam E-Mail Traffic Share 2019.” n.d. <em>Statista</em>. Accessed January 10, 2021. <a href="https://www.statista.com/statistics/420391/spam-email-traffic-share/">https://www.statista.com/statistics/420391/spam-email-traffic-share/</a>.</p>
</div>
<div id="ref-noauthor_uci_spam_nodate">
<p>“UCI Machine Learning Repository: SMS Spam Collection Data Set.” n.d. Accessed January 9, 2021. <a href="https://archive.ics.uci.edu/ml/datasets/sms+spam+collection">https://archive.ics.uci.edu/ml/datasets/sms+spam+collection</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="knn.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="references.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["gitbook.pdf", "gitbook.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
