[["index.html", "Data Mining Preface Prerequisites Purpose of this course Structure of the course", " Data Mining Witek ten hove 2021-02-18 Preface Figure 0.1: CRISP-DM Model taken from: https://commons.wikimedia.org/wiki/File:CRISP-DM_Process_Diagram.png Prerequisites Before starting this module make sure you have: access to the book Provost, F., &amp; Fawcett, T. (2013). Data Science for Business: What you need to know about data mining and data-analytic thinking. O’Reilly Media, Inc. installed R and RStudio a Github account Purpose of this course The general learning outcome of this course is: The student is able to perform a well-defined task independently in a relatively clearly arranged situation, or is able to perform in a complex and unpredictable situation under supervision. The course will provide you with a few essential data mining skills. The focus will lie on non-linear modeling techniques - k-Nearest Neighbors (kNN) and Naive Bayes classification. After a successful completion of the course, a student: is able to prepare data for a given non-linear model train en test a non-linear model evaluate the quality of a trained model Structure of the course Table 0.1: Course overview Week nr. Module name Readings 2 Onboarding and Introduction to the Course Provost / Fawcett Ch.3 3-4 Lazy Learning with kNN Provost / Fawcett Ch.6 + 7 5-6 Probabilistic Kearning with Naive Bayes classification Provost / Fawcett Ch.9 7 Project Application Through the whole of the program you’ll be cooperating within a team where you will combine and compare the results of the different case studies. At the end of the course you will present with your team what you have learned from analyzing and comparing the different case studies. "],["about-the-author.html", "About the author", " About the author Witek ten Hove is a senior instructor and researcher at HAN University of Applied Sciences. His main areas of expertise are Data en Web Technologies. Through his extensive business experience in Finance and International Trade and thorough knowledge of modern data technologies, he is able to make connections between technology and business. As an open source evangelist he firmly believe in the power of knowledge sharing. His mission is to inspire business professionals and help them exploit the full potential of smart technologies. He is the owner of Ten Hove Business Data Solutions, a consultancy and training company helping organizations to achieve maximum business value through data driven solutions. "],["dm.html", "Module 1 Introduction 1.1 Working with RMarkdown 1.2 Working with Git and Github 1.3 Customizing RMarkdown with HTML and CSS", " Module 1 Introduction 1.1 Working with RMarkdown 1.2 Working with Git and Github 1.3 Customizing RMarkdown with HTML and CSS "],["knn.html", "Module 2 Lazy learning with k-Nearest Neighbors 2.1 Business Case: Diagnosing Breast Cancer 2.2 Data Understanding 2.3 Preparation 2.4 Modeling and Evaluation", " Module 2 Lazy learning with k-Nearest Neighbors 2.1 Business Case: Diagnosing Breast Cancer Breast cancer is the top cancer in women both in the developed and the developing world. In the Netherlands it is the most pervasive form of cancer (“WHO Cancer Country Profiles 2020” n.d.). In order to improve breast cancer outcome and survival early detection remains the most important instrument for breast cancer control. If machine learning could automate the identification of cancer, it would improve efficiency of the detection process and might also increase its effectiveness by providing greater detection accuracy. 2.2 Data Understanding The data we will be using comes from the University of Wisconsin and is available online as an open source dataset (“UCI Machine Learning Repository: Breast Cancer Wisconsin (Diagnostic) Data Set” n.d.). It includes measurements from digitized images from from fine-needle aspirates of breast mass. The values represent cell nuclei features. For convenience the data in csv format is stored on Github. We can access it directly using a function dedicated to reading csv from the readr package. url &lt;- &quot;https://raw.githubusercontent.com/businessdatasolutions/courses/main/data%20mining/gitbook/datasets/breastcancer.csv&quot; rawDF &lt;- read_csv(url) Using the str() function we can have some basic information about the dataset. str(rawDF) ## tibble [569 × 32] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ id : num [1:569] 87139402 8910251 905520 868871 9012568 ... ## $ diagnosis : chr [1:569] &quot;B&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; ... ## $ radius_mean : num [1:569] 12.3 10.6 11 11.3 15.2 ... ## $ texture_mean : num [1:569] 12.4 18.9 16.8 13.4 13.2 ... ## $ perimeter_mean : num [1:569] 78.8 69.3 70.9 73 97.7 ... ## $ area_mean : num [1:569] 464 346 373 385 712 ... ## $ smoothness_mean : num [1:569] 0.1028 0.0969 0.1077 0.1164 0.0796 ... ## $ compactness_mean : num [1:569] 0.0698 0.1147 0.078 0.1136 0.0693 ... ## $ concavity_mean : num [1:569] 0.0399 0.0639 0.0305 0.0464 0.0339 ... ## $ points_mean : num [1:569] 0.037 0.0264 0.0248 0.048 0.0266 ... ## $ symmetry_mean : num [1:569] 0.196 0.192 0.171 0.177 0.172 ... ## $ dimension_mean : num [1:569] 0.0595 0.0649 0.0634 0.0607 0.0554 ... ## $ radius_se : num [1:569] 0.236 0.451 0.197 0.338 0.178 ... ## $ texture_se : num [1:569] 0.666 1.197 1.387 1.343 0.412 ... ## $ perimeter_se : num [1:569] 1.67 3.43 1.34 1.85 1.34 ... ## $ area_se : num [1:569] 17.4 27.1 13.5 26.3 17.7 ... ## $ smoothness_se : num [1:569] 0.00805 0.00747 0.00516 0.01127 0.00501 ... ## $ compactness_se : num [1:569] 0.0118 0.03581 0.00936 0.03498 0.01485 ... ## $ concavity_se : num [1:569] 0.0168 0.0335 0.0106 0.0219 0.0155 ... ## $ points_se : num [1:569] 0.01241 0.01365 0.00748 0.01965 0.00915 ... ## $ symmetry_se : num [1:569] 0.0192 0.035 0.0172 0.0158 0.0165 ... ## $ dimension_se : num [1:569] 0.00225 0.00332 0.0022 0.00344 0.00177 ... ## $ radius_worst : num [1:569] 13.5 11.9 12.4 11.9 16.2 ... ## $ texture_worst : num [1:569] 15.6 22.9 26.4 15.8 15.7 ... ## $ perimeter_worst : num [1:569] 87 78.3 79.9 76.5 104.5 ... ## $ area_worst : num [1:569] 549 425 471 434 819 ... ## $ smoothness_worst : num [1:569] 0.139 0.121 0.137 0.137 0.113 ... ## $ compactness_worst: num [1:569] 0.127 0.252 0.148 0.182 0.174 ... ## $ concavity_worst : num [1:569] 0.1242 0.1916 0.1067 0.0867 0.1362 ... ## $ points_worst : num [1:569] 0.0939 0.0793 0.0743 0.0861 0.0818 ... ## $ symmetry_worst : num [1:569] 0.283 0.294 0.3 0.21 0.249 ... ## $ dimension_worst : num [1:569] 0.0677 0.0759 0.0788 0.0678 0.0677 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. id = col_double(), ## .. diagnosis = col_character(), ## .. radius_mean = col_double(), ## .. texture_mean = col_double(), ## .. perimeter_mean = col_double(), ## .. area_mean = col_double(), ## .. smoothness_mean = col_double(), ## .. compactness_mean = col_double(), ## .. concavity_mean = col_double(), ## .. points_mean = col_double(), ## .. symmetry_mean = col_double(), ## .. dimension_mean = col_double(), ## .. radius_se = col_double(), ## .. texture_se = col_double(), ## .. perimeter_se = col_double(), ## .. area_se = col_double(), ## .. smoothness_se = col_double(), ## .. compactness_se = col_double(), ## .. concavity_se = col_double(), ## .. points_se = col_double(), ## .. symmetry_se = col_double(), ## .. dimension_se = col_double(), ## .. radius_worst = col_double(), ## .. texture_worst = col_double(), ## .. perimeter_worst = col_double(), ## .. area_worst = col_double(), ## .. smoothness_worst = col_double(), ## .. compactness_worst = col_double(), ## .. concavity_worst = col_double(), ## .. points_worst = col_double(), ## .. symmetry_worst = col_double(), ## .. dimension_worst = col_double() ## .. ) The dataset has 32 variables (columns) and 569 observations (rows). 2.3 Preparation The first variable, id, contains unique patient IDs. The IDs do not contain any relevant information for making predictions, so we will delete it from the dataset. cleanDF &lt;- rawDF[-1] head(cleanDF) ## # A tibble: 6 x 31 ## diagnosis radius_mean texture_mean perimeter_mean area_mean smoothness_mean ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 B 12.3 12.4 78.8 464. 0.103 ## 2 B 10.6 19.0 69.3 346. 0.0969 ## 3 B 11.0 16.8 70.9 373. 0.108 ## 4 B 11.3 13.4 73 385. 0.116 ## 5 B 15.2 13.2 97.6 712. 0.0796 ## 6 B 11.6 19.0 74.2 410. 0.0855 ## # … with 25 more variables: compactness_mean &lt;dbl&gt;, concavity_mean &lt;dbl&gt;, ## # points_mean &lt;dbl&gt;, symmetry_mean &lt;dbl&gt;, dimension_mean &lt;dbl&gt;, ## # radius_se &lt;dbl&gt;, texture_se &lt;dbl&gt;, perimeter_se &lt;dbl&gt;, area_se &lt;dbl&gt;, ## # smoothness_se &lt;dbl&gt;, compactness_se &lt;dbl&gt;, concavity_se &lt;dbl&gt;, ## # points_se &lt;dbl&gt;, symmetry_se &lt;dbl&gt;, dimension_se &lt;dbl&gt;, radius_worst &lt;dbl&gt;, ## # texture_worst &lt;dbl&gt;, perimeter_worst &lt;dbl&gt;, area_worst &lt;dbl&gt;, ## # smoothness_worst &lt;dbl&gt;, compactness_worst &lt;dbl&gt;, concavity_worst &lt;dbl&gt;, ## # points_worst &lt;dbl&gt;, symmetry_worst &lt;dbl&gt;, dimension_worst &lt;dbl&gt; The variable named diagnosis contains the outcomes we would like to predict - ‘B’ for ‘Benign’ and ‘M’ for ‘Malignant.’ The variable we would like to predict is called the ‘label.’ We can look at the counts and proportions for both outcomes, using the tables() and prop.tables()functions. cntDiag &lt;- table(cleanDF$diagnosis) propDiag &lt;- round(prop.table(cntDiag) * 100 , digits = 1) cntDiag ## ## B M ## 357 212 propDiag ## ## B M ## 62.7 37.3 The variable is now coded as a type character. Many models require that the label is of type factor. This is easily solved using the factor() function. cleanDF$diagnosis &lt;- factor(cleanDF$diagnosis, levels = c(&quot;B&quot;, &quot;M&quot;), labels = c(&quot;Benign&quot;, &quot;Malignant&quot;)) %&gt;% relevel(&quot;Malignant&quot;) head(cleanDF, 10) ## # A tibble: 10 x 31 ## diagnosis radius_mean texture_mean perimeter_mean area_mean smoothness_mean ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Benign 12.3 12.4 78.8 464. 0.103 ## 2 Benign 10.6 19.0 69.3 346. 0.0969 ## 3 Benign 11.0 16.8 70.9 373. 0.108 ## 4 Benign 11.3 13.4 73 385. 0.116 ## 5 Benign 15.2 13.2 97.6 712. 0.0796 ## 6 Benign 11.6 19.0 74.2 410. 0.0855 ## 7 Benign 11.5 23.9 74.5 404. 0.0926 ## 8 Malignant 13.8 23.8 91.6 598. 0.132 ## 9 Benign 10.5 19.3 67.4 336. 0.0999 ## 10 Benign 11.1 15.0 71.5 374. 0.103 ## # … with 25 more variables: compactness_mean &lt;dbl&gt;, concavity_mean &lt;dbl&gt;, ## # points_mean &lt;dbl&gt;, symmetry_mean &lt;dbl&gt;, dimension_mean &lt;dbl&gt;, ## # radius_se &lt;dbl&gt;, texture_se &lt;dbl&gt;, perimeter_se &lt;dbl&gt;, area_se &lt;dbl&gt;, ## # smoothness_se &lt;dbl&gt;, compactness_se &lt;dbl&gt;, concavity_se &lt;dbl&gt;, ## # points_se &lt;dbl&gt;, symmetry_se &lt;dbl&gt;, dimension_se &lt;dbl&gt;, radius_worst &lt;dbl&gt;, ## # texture_worst &lt;dbl&gt;, perimeter_worst &lt;dbl&gt;, area_worst &lt;dbl&gt;, ## # smoothness_worst &lt;dbl&gt;, compactness_worst &lt;dbl&gt;, concavity_worst &lt;dbl&gt;, ## # points_worst &lt;dbl&gt;, symmetry_worst &lt;dbl&gt;, dimension_worst &lt;dbl&gt; The features consist of three different measurements of ten characteristics. We will take three characteristics and have a closer look. summary(cleanDF[c(&quot;radius_mean&quot;, &quot;area_mean&quot;, &quot;smoothness_mean&quot;)]) ## radius_mean area_mean smoothness_mean ## Min. : 6.981 Min. : 143.5 Min. :0.05263 ## 1st Qu.:11.700 1st Qu.: 420.3 1st Qu.:0.08637 ## Median :13.370 Median : 551.1 Median :0.09587 ## Mean :14.127 Mean : 654.9 Mean :0.09636 ## 3rd Qu.:15.780 3rd Qu.: 782.7 3rd Qu.:0.10530 ## Max. :28.110 Max. :2501.0 Max. :0.16340 You’ll notice that the three variables have very different ranges and as a consequence area_mean will have a larger impact on the distance calculation than the smootness_mean. This could potentially cause problems for modeling. To solve this we’ll apply normalization to rescale all features to a standard range of values. We will write our own normalization function. normalize &lt;- function(x) { # Function takes in a vector return ((x - min(x)) / (max(x) - min(x))) # distance of item value - minimum vector value divided by the range of all vector values } testSet1 &lt;- c(1:5) testSet2 &lt;- c(1:5) * 10 cat(&quot;testSet1:&quot;, testSet1, &quot;\\n&quot;) ## testSet1: 1 2 3 4 5 cat(&quot;testSet2:&quot;, testSet2, &quot;\\n&quot;) ## testSet2: 10 20 30 40 50 cat(&quot;Normalized testSet1:&quot;, normalize(testSet1), &quot;\\n&quot;) ## Normalized testSet1: 0 0.25 0.5 0.75 1 cat(&quot;Normalized testSet2:&quot;, normalize(testSet2)) ## Normalized testSet2: 0 0.25 0.5 0.75 1 We’ll apply the normalize() function to each feature in the dataset (so, not on the label) using the sapply() function. nCols &lt;- dim(cleanDF)[2] cleanDF_n &lt;- sapply(2:nCols, function(x) { normalize(cleanDF[,x]) }) %&gt;% as.data.frame() summary(cleanDF_n[c(&quot;radius_mean&quot;, &quot;area_mean&quot;, &quot;smoothness_mean&quot;)]) ## radius_mean area_mean smoothness_mean ## Min. :0.0000 Min. :0.0000 Min. :0.0000 ## 1st Qu.:0.2233 1st Qu.:0.1174 1st Qu.:0.3046 ## Median :0.3024 Median :0.1729 Median :0.3904 ## Mean :0.3382 Mean :0.2169 Mean :0.3948 ## 3rd Qu.:0.4164 3rd Qu.:0.2711 3rd Qu.:0.4755 ## Max. :1.0000 Max. :1.0000 Max. :1.0000 When we take the variables we selected earlier and look at the summary parameters again, we’ll see that the normalization was successful. We can now split our data into training and test sets. trainDF_feat &lt;- cleanDF_n[1:469, ] testDF_feat &lt;- cleanDF_n[470:569, ] When creating the training and test sets, we’ve excluded the labels. We’ll create separate training and tests sets for them too. trainDF_labels &lt;- cleanDF[1:469, 1] testDF_labels &lt;- cleanDF[470:569, 1] Now we can train and evaluate our kNN model. 2.4 Modeling and Evaluation To train the knn model we only need one single function from the class package. It takes the set with training features and the set with training label. The trained model is applied to the set with test features and the function gives back a set of predictions. cleanDF_test_pred &lt;- knn(train = as.matrix(trainDF_feat), test = as.matrix(testDF_feat), cl = as.matrix(trainDF_labels), k = 21) head(cleanDF_test_pred) ## [1] Benign Benign Benign Benign Malignant Benign ## Levels: Benign Malignant Now that we have a set of predicted labels we can compare these with the actual labels. A diffusion table shows how well the model performed. Figure 2.1: Standard diffusion table. Taken from: https://emj.bmj.com/content/emermed/36/7/431/F1.large.jpg Here is our own table: confusionMatrix(cleanDF_test_pred, testDF_labels[[1]], positive = NULL, dnn = c(&quot;Prediction&quot;, &quot;True&quot;)) ## Warning in confusionMatrix.default(cleanDF_test_pred, testDF_labels[[1]], : ## Levels are not in the same order for reference and data. Refactoring data to ## match. ## Confusion Matrix and Statistics ## ## True ## Prediction Malignant Benign ## Malignant 37 0 ## Benign 2 61 ## ## Accuracy : 0.98 ## 95% CI : (0.9296, 0.9976) ## No Information Rate : 0.61 ## P-Value [Acc &gt; NIR] : &lt;2e-16 ## ## Kappa : 0.9576 ## ## Mcnemar&#39;s Test P-Value : 0.4795 ## ## Sensitivity : 0.9487 ## Specificity : 1.0000 ## Pos Pred Value : 1.0000 ## Neg Pred Value : 0.9683 ## Prevalence : 0.3900 ## Detection Rate : 0.3700 ## Detection Prevalence : 0.3700 ## Balanced Accuracy : 0.9744 ## ## &#39;Positive&#39; Class : Malignant ## Questions: How would you assess the overall performance of the model? What would you consider as more costly: high false negatives or high false positives levels? Why? "],["naivebayes.html", "Module 3 Probabilistic Learning with Naive Bayes Classification 3.1 Business Case: Filtering Spam 3.2 Data Understanding 3.3 Preparation 3.4 Modeling and Evaluation", " Module 3 Probabilistic Learning with Naive Bayes Classification 3.1 Business Case: Filtering Spam In 2020 spam account for more than 50% of total e-mail traffic (“Spam Statistics: Spam e-Mail Traffic Share 2019” n.d.). This illustrates the value of a good spam filter. Naive Bayes spam filtering is a standard technique for handling spam. It is one of the oldest ways of doing spam filtering, with roots in the 1990s. 3.2 Data Understanding The data we’ll use comes from the SMS Spam Collection (“UCI Machine Learning Repository: SMS Spam Collection Data Set” n.d.). It contains a set SMS messages that are label ‘ham’ or ‘spam.’ and is a standard data set for testing spam filtering methods. url &lt;- &quot;datasets/smsspam.csv&quot; rawDF &lt;- read_csv(url) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## type = col_character(), ## text = col_character() ## ) head(rawDF) ## # A tibble: 6 x 2 ## type text ## &lt;chr&gt; &lt;chr&gt; ## 1 ham Go until jurong point, crazy.. Available only in bugis n great world la… ## 2 ham Ok lar... Joking wif u oni... ## 3 spam Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Tex… ## 4 ham U dun say so early hor... U c already then say... ## 5 ham Nah I don&#39;t think he goes to usf, he lives around here though ## 6 spam FreeMsg Hey there darling it&#39;s been 3 week&#39;s now and no word back! I&#39;d … The dataset has 2 variables (columns) and 5572 observations (rows). The variable type is of class character. As it indicates whether the message belongs to the category ham or spam we should convert it to a factor variable. rawDF$type &lt;- rawDF$type %&gt;% factor %&gt;% relevel(&quot;spam&quot;) class(rawDF$type) ## [1] &quot;factor&quot; We can also visually inspect the data by creating wordclouds for each sms type. spam &lt;- rawDF %&gt;% filter(type == &quot;spam&quot;) ham &lt;- rawDF %&gt;% filter(type == &quot;ham&quot;) wordcloud(spam$text, max.words = 20, scale = c(4, 0.8), colors= c(&quot;indianred1&quot;,&quot;indianred2&quot;,&quot;indianred3&quot;,&quot;indianred&quot;)) wordcloud(ham$text, max.words = 20, scale = c(4, 0.8), colors= c(&quot;lightsteelblue1&quot;,&quot;lightsteelblue2&quot;,&quot;lightsteelblue3&quot;,&quot;lightsteelblue&quot;)) Question: What differences do you notice? 3.3 Preparation First we need to create a corpus, which refers to a collection of text documents. In our case each sms is considered a text document. We’ll use the Corpus() function from thetm package. rawCorpus &lt;- Corpus(VectorSource(rawDF$text)) inspect(rawCorpus[1:3]) ## &lt;&lt;SimpleCorpus&gt;&gt; ## Metadata: corpus specific: 1, document level (indexed): 0 ## Content: documents: 3 ## ## [1] Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat... ## [2] Ok lar... Joking wif u oni... ## [3] Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C&#39;s apply 08452810075over18&#39;s The corpus contains 5572 documents. Which obviously matches with the number of rows in our dataset. We will use the function tm_map() to do some first cleaning up. First we’ll change everything to lowercase. We’ll also remove numbers as these will contain litle information on a message being spam or not. cleanCorpus &lt;- rawCorpus %&gt;% tm_map(tolower) %&gt;% tm_map(removeNumbers) ## Warning in tm_map.SimpleCorpus(., tolower): transformation drops documents ## Warning in tm_map.SimpleCorpus(., removeNumbers): transformation drops documents For computation efficiency it is important to eliminate all items from a dataset of which you’re rather confident that they’ do’ll add little information to your model. In our case we can expect that words like “and” or “but” will be equally common in both ham and spam messages. We should therefore filter them out before we start modeling. We’ll also remove punctuation. cleanCorpus &lt;- cleanCorpus %&gt;% tm_map(tolower) %&gt;% tm_map(removeWords, stopwords()) %&gt;% tm_map(removePunctuation) ## Warning in tm_map.SimpleCorpus(., tolower): transformation drops documents ## Warning in tm_map.SimpleCorpus(., removeWords, stopwords()): transformation ## drops documents ## Warning in tm_map.SimpleCorpus(., removePunctuation): transformation drops ## documents Now that we have removed certain items, the text lines contain a lot of whitespaces where these items used to be. In our last step we will remove additional whitespace. cleanCorpus &lt;- cleanCorpus %&gt;% tm_map(stripWhitespace) ## Warning in tm_map.SimpleCorpus(., stripWhitespace): transformation drops ## documents Let’s inspect the corpus again. Compare it to the raw version. tibble(Raw = rawCorpus$content[1:3], Clean = cleanCorpus$content[1:3]) ## # A tibble: 3 x 2 ## Raw Clean ## &lt;chr&gt; &lt;chr&gt; ## 1 Go until jurong point, crazy.. Availab… go jurong point crazy available bugis… ## 2 Ok lar... Joking wif u oni... ok lar joking wif u oni ## 3 Free entry in 2 a wkly comp to win FA … free entry wkly comp win fa cup final… Now that we have cleaned up the texts, we are going to transform the messages to a matrix. Each word in the each message will get its own column, each row will be a message and the cells of the matrix will contain a word count. cleanDTM &lt;- cleanCorpus %&gt;% DocumentTermMatrix inspect(cleanDTM) ## &lt;&lt;DocumentTermMatrix (documents: 5572, terms: 7829)&gt;&gt; ## Non-/sparse entries: 42822/43580366 ## Sparsity : 100% ## Maximal term length: 40 ## Weighting : term frequency (tf) ## Sample : ## Terms ## Docs call can free get just know like ltgt now will ## 1085 0 0 0 1 0 0 1 0 0 11 ## 1579 0 0 0 0 0 0 0 18 0 0 ## 1863 0 0 0 0 0 1 1 0 0 0 ## 2158 0 0 0 0 0 0 0 0 0 0 ## 2370 0 0 1 0 0 0 1 0 0 0 ## 2380 0 1 0 0 0 0 0 1 0 0 ## 2434 0 3 1 1 0 0 0 6 0 0 ## 2848 0 0 0 0 0 0 0 0 0 0 ## 3016 0 0 0 0 0 0 0 2 0 0 ## 5105 0 0 1 0 0 0 1 0 0 0 Before we start modeling we need to split all datasets into train and test sets. For this we will use a function from the caret package. The function createDataPartition() can be used to create balanced splits of the data. If the y argument to this function is a factor, the random sampling occurs within each class and should preserve the overall class distribution of the data. In this case we’ll create a 75/25% split. # Create split indices set.seed(1234) trainIndex &lt;- createDataPartition(rawDF$type, p = .75, list = FALSE, times = 1) head(trainIndex) ## Resample1 ## [1,] 3 ## [2,] 4 ## [3,] 5 ## [4,] 6 ## [5,] 7 ## [6,] 10 # Apply split indices to DF trainDF &lt;- rawDF[trainIndex, ] ## Warning: The `i` argument of ``[`()` can&#39;t be a matrix as of tibble 3.0.0. ## Convert to a vector. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_warnings()` to see where this warning was generated. testDF &lt;- rawDF[-trainIndex, ] # Apply split indices to Corpus trainCorpus &lt;- cleanCorpus[trainIndex] testCorpus &lt;- cleanCorpus[-trainIndex] # Apply split indices to DTM trainDTM &lt;- cleanDTM[trainIndex, ] testDTM &lt;- cleanDTM[-trainIndex, ] As you can check (how?) the DTM has almost 7800 features. Remember that each feature in the DTM is a word. Some words will have very few counts and therefore will have limited predictive power. To save on computation time we will eliminate words with low frequencies. freqWords &lt;- trainDTM %&gt;% findFreqTerms(5) trainDTM &lt;- DocumentTermMatrix(trainCorpus, list(dictionary = freqWords)) testDTM &lt;- DocumentTermMatrix(testCorpus, list(dictionary = freqWords)) With this operation we’ve reduced the number of features to around 1200. Another issue is that the Naive Bayes classifier is typically trained on categorical features. We now have numerical matrix with word counts. We will transform the counts into a factor that simply indicates whether the word appears in the document or not. We’ll first build our own function for this and then apply it to each column in the DTM. convert_counts &lt;- function(x) { x &lt;- ifelse(x &gt; 0, 1, 0) %&gt;% factor(levels = c(0,1), labels = c(&quot;No&quot;, &quot;Yes&quot;)) } nColsDTM &lt;- dim(trainDTM)[2] trainDTM &lt;- apply(trainDTM, MARGIN = 2, convert_counts) testDTM &lt;- apply(testDTM, MARGIN = 2, convert_counts) head(trainDTM[,1:10]) ## Terms ## Docs apply comp cup entry final free may receive text txt ## 1 &quot;Yes&quot; &quot;Yes&quot; &quot;Yes&quot; &quot;Yes&quot; &quot;Yes&quot; &quot;Yes&quot; &quot;Yes&quot; &quot;Yes&quot; &quot;Yes&quot; &quot;Yes&quot; ## 2 &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; ## 3 &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; ## 4 &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; ## 5 &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; ## 6 &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;Yes&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; 3.4 Modeling and Evaluation We have now everything in place to start training our model and evaluate against our test dataset. The naiveBayes() function is part of the e1071 package. It takes in the features and labels of our training dataset and returns a trained model. nbayesModel &lt;- naiveBayes(trainDTM, trainDF$type, laplace = 1) The model van be applied to the test features using the predict() funtion which generates a vector of predictions. Using a confusion matrix we can analyze the performance of our model. Figure 3.1: Standard diffusion table. Taken from: https://emj.bmj.com/content/emermed/36/7/431/F1.large.jpg predVec &lt;- predict(nbayesModel, testDTM) confusionMatrix(predVec, testDF$type, positive = &quot;spam&quot;, dnn = c(&quot;Prediction&quot;, &quot;True&quot;)) ## Confusion Matrix and Statistics ## ## True ## Prediction spam ham ## spam 173 26 ## ham 13 1180 ## ## Accuracy : 0.972 ## 95% CI : (0.9619, 0.98) ## No Information Rate : 0.8664 ## P-Value [Acc &gt; NIR] : &lt; 2e-16 ## ## Kappa : 0.8825 ## ## Mcnemar&#39;s Test P-Value : 0.05466 ## ## Sensitivity : 0.9301 ## Specificity : 0.9784 ## Pos Pred Value : 0.8693 ## Neg Pred Value : 0.9891 ## Prevalence : 0.1336 ## Detection Rate : 0.1243 ## Detection Prevalence : 0.1430 ## Balanced Accuracy : 0.9543 ## ## &#39;Positive&#39; Class : spam ## Questions: What do you think is the role of the laplace parameter in the naiveBayes() function? How would you assess the overall performance of the model? What would you consider as more costly: high false negatives or high false positives levels? Why? "],["references.html", "References", " References “Spam Statistics: Spam e-Mail Traffic Share 2019.” n.d. Statista. Accessed January 10, 2021. https://www.statista.com/statistics/420391/spam-email-traffic-share/. “UCI Machine Learning Repository: Breast Cancer Wisconsin (Diagnostic) Data Set.” n.d. Accessed January 7, 2021. https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic). “UCI Machine Learning Repository: SMS Spam Collection Data Set.” n.d. Accessed January 9, 2021. https://archive.ics.uci.edu/ml/datasets/sms+spam+collection. “WHO Cancer Country Profiles 2020.” n.d. WHO. Accessed January 7, 2021. http://www.who.int/cancer/country-profiles/en/. "]]
