[["knn.html", "Module 2 Lazy learning with k-Nearest Neighbors 2.1 Business Case: Diagnosing Breast Cancer 2.2 Data Understanding 2.3 Preparation 2.4 Modeling 2.5 Evaluation 2.6 Deployment", " Module 2 Lazy learning with k-Nearest Neighbors 2.1 Business Case: Diagnosing Breast Cancer Breast cancer is the top cancer in women both in the developed and the developing world. In the Netherlands it is the most pervasive form of cancer (“WHO Cancer Country Profiles 2020” n.d.). In order to improve breast cancer outcome and survival early detection remains the most important instrument for breast cancer control. If machine learning could automate the identification of cancer, it would improve efficiency of the detection process and might also increase its effectiveness by providing greater detection accuracy. 2.2 Data Understanding The data we will be using comes from the University of Wisconsin and is available online as an open source dataset (“UCI Machine Learning Repository: Breast Cancer Wisconsin (Diagnostic) Data Set” n.d.). It includes measurements from digitized images from from fine-needle aspirates of breast mass. The values represent cell nuclei features. The data is saved online as a Google Spreadsheet. We can access it directly using a function dedicated to reading Google Spreadsheets from the googlesheets4 package. gs4_deauth() rawDF &lt;- read_sheet(&quot;https://docs.google.com/spreadsheets/d/1vnrj4nylo1X_yM3DBi-oQhBb2Cb2eVlbOdyWsxiqI9g/edit?usp=sharing&quot;) ## Reading from &quot;Breast cancer&quot; ## Range &quot;Sheet1&quot; Using the str() function we can have some basic information about the dataset. str(rawDF) ## tibble [569 × 32] (S3: tbl_df/tbl/data.frame) ## $ id : num [1:569] 87139402 8910251 905520 868871 9012568 ... ## $ diagnosis : chr [1:569] &quot;B&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; ... ## $ radius_mean : num [1:569] 12.3 10.6 11 11.3 15.2 ... ## $ texture_mean : num [1:569] 12.4 18.9 16.8 13.4 13.2 ... ## $ perimeter_mean : num [1:569] 78.8 69.3 70.9 73 97.7 ... ## $ area_mean : num [1:569] 464 346 373 385 712 ... ## $ smoothness_mean : num [1:569] 0.1028 0.0969 0.1077 0.1164 0.0796 ... ## $ compactness_mean : num [1:569] 0.0698 0.1147 0.078 0.1136 0.0693 ... ## $ concavity_mean : num [1:569] 0.0399 0.0639 0.0305 0.0464 0.0339 ... ## $ points_mean : num [1:569] 0.037 0.0264 0.0248 0.048 0.0266 ... ## $ symmetry_mean : num [1:569] 0.196 0.192 0.171 0.177 0.172 ... ## $ dimension_mean : num [1:569] 0.0595 0.0649 0.0634 0.0607 0.0554 ... ## $ radius_se : num [1:569] 0.236 0.451 0.197 0.338 0.178 ... ## $ texture_se : num [1:569] 0.666 1.197 1.387 1.343 0.412 ... ## $ perimeter_se : num [1:569] 1.67 3.43 1.34 1.85 1.34 ... ## $ area_se : num [1:569] 17.4 27.1 13.5 26.3 17.7 ... ## $ smoothness_se : num [1:569] 0.00805 0.00747 0.00516 0.01127 0.00501 ... ## $ compactness_se : num [1:569] 0.0118 0.03581 0.00936 0.03498 0.01485 ... ## $ concavity_se : num [1:569] 0.0168 0.0335 0.0106 0.0219 0.0155 ... ## $ points_se : num [1:569] 0.01241 0.01365 0.00748 0.01965 0.00915 ... ## $ symmetry_se : num [1:569] 0.0192 0.035 0.0172 0.0158 0.0165 ... ## $ dimension_se : num [1:569] 0.00225 0.00332 0.0022 0.00344 0.00177 ... ## $ radius_worst : num [1:569] 13.5 11.9 12.4 11.9 16.2 ... ## $ texture_worst : num [1:569] 15.6 22.9 26.4 15.8 15.7 ... ## $ perimeter_worst : num [1:569] 87 78.3 79.9 76.5 104.5 ... ## $ area_worst : num [1:569] 549 425 471 434 819 ... ## $ smoothness_worst : num [1:569] 0.139 0.121 0.137 0.137 0.113 ... ## $ compactness_worst: num [1:569] 0.127 0.252 0.148 0.182 0.174 ... ## $ concavity_worst : num [1:569] 0.1242 0.1916 0.1067 0.0867 0.1362 ... ## $ points_worst : num [1:569] 0.0939 0.0793 0.0743 0.0861 0.0818 ... ## $ symmetry_worst : num [1:569] 0.283 0.294 0.3 0.21 0.249 ... ## $ dimension_worst : num [1:569] 0.0677 0.0759 0.0788 0.0678 0.0677 ... The dataset has 32 variables (columns) and 569 observations (rows). 2.3 Preparation The first variable, id, contains unique patient IDs. The IDs do not contain any relevant information for making predictions, so we will delete it from the dataset. cleanDF &lt;- rawDF[-1] head(cleanDF) ## # A tibble: 6 x 31 ## diagnosis radius_mean texture_mean perimeter_mean area_mean smoothness_mean ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 B 12.3 12.4 78.8 464. 0.103 ## 2 B 10.6 19.0 69.3 346. 0.0969 ## 3 B 11.0 16.8 70.9 373. 0.108 ## 4 B 11.3 13.4 73 385. 0.116 ## 5 B 15.2 13.2 97.6 712. 0.0796 ## 6 B 11.6 19.0 74.2 410. 0.0855 ## # … with 25 more variables: compactness_mean &lt;dbl&gt;, concavity_mean &lt;dbl&gt;, ## # points_mean &lt;dbl&gt;, symmetry_mean &lt;dbl&gt;, dimension_mean &lt;dbl&gt;, ## # radius_se &lt;dbl&gt;, texture_se &lt;dbl&gt;, perimeter_se &lt;dbl&gt;, area_se &lt;dbl&gt;, ## # smoothness_se &lt;dbl&gt;, compactness_se &lt;dbl&gt;, concavity_se &lt;dbl&gt;, ## # points_se &lt;dbl&gt;, symmetry_se &lt;dbl&gt;, dimension_se &lt;dbl&gt;, radius_worst &lt;dbl&gt;, ## # texture_worst &lt;dbl&gt;, perimeter_worst &lt;dbl&gt;, area_worst &lt;dbl&gt;, ## # smoothness_worst &lt;dbl&gt;, compactness_worst &lt;dbl&gt;, concavity_worst &lt;dbl&gt;, ## # points_worst &lt;dbl&gt;, symmetry_worst &lt;dbl&gt;, dimension_worst &lt;dbl&gt; The variable named diagnosis contains the outcomes we would like to predict - ‘B’ for ‘Benign’ and ‘M’ for ‘Malignant’. The variable we would like to predict is called the ‘label’. We can look at the counts and proportions for both outcomes, using the tables() and prop.tables()functions. cntDiag &lt;- table(cleanDF$diagnosis) propDiag &lt;- round(prop.table(cntDiag) * 100 , digits = 1) cntDiag ## ## B M ## 357 212 propDiag ## ## B M ## 62.7 37.3 The variable is now coded as a type character. Many models require that the label is of type factor. This is easily solved using the factor() function. cleanDF$diagnosis &lt;- factor(cleanDF$diagnosis, levels = c(&quot;B&quot;, &quot;M&quot;), labels = c(&quot;Benign&quot;, &quot;Malignant&quot;)) head(cleanDF, 10) ## # A tibble: 10 x 31 ## diagnosis radius_mean texture_mean perimeter_mean area_mean smoothness_mean ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Benign 12.3 12.4 78.8 464. 0.103 ## 2 Benign 10.6 19.0 69.3 346. 0.0969 ## 3 Benign 11.0 16.8 70.9 373. 0.108 ## 4 Benign 11.3 13.4 73 385. 0.116 ## 5 Benign 15.2 13.2 97.6 712. 0.0796 ## 6 Benign 11.6 19.0 74.2 410. 0.0855 ## 7 Benign 11.5 23.9 74.5 404. 0.0926 ## 8 Malignant 13.8 23.8 91.6 598. 0.132 ## 9 Benign 10.5 19.3 67.4 336. 0.0999 ## 10 Benign 11.1 15.0 71.5 374. 0.103 ## # … with 25 more variables: compactness_mean &lt;dbl&gt;, concavity_mean &lt;dbl&gt;, ## # points_mean &lt;dbl&gt;, symmetry_mean &lt;dbl&gt;, dimension_mean &lt;dbl&gt;, ## # radius_se &lt;dbl&gt;, texture_se &lt;dbl&gt;, perimeter_se &lt;dbl&gt;, area_se &lt;dbl&gt;, ## # smoothness_se &lt;dbl&gt;, compactness_se &lt;dbl&gt;, concavity_se &lt;dbl&gt;, ## # points_se &lt;dbl&gt;, symmetry_se &lt;dbl&gt;, dimension_se &lt;dbl&gt;, radius_worst &lt;dbl&gt;, ## # texture_worst &lt;dbl&gt;, perimeter_worst &lt;dbl&gt;, area_worst &lt;dbl&gt;, ## # smoothness_worst &lt;dbl&gt;, compactness_worst &lt;dbl&gt;, concavity_worst &lt;dbl&gt;, ## # points_worst &lt;dbl&gt;, symmetry_worst &lt;dbl&gt;, dimension_worst &lt;dbl&gt; The features consist of three different measurements of ten characteristics. We will take three characteristics and have a closer look. summary(cleanDF[c(&quot;radius_mean&quot;, &quot;area_mean&quot;, &quot;smoothness_mean&quot;)]) ## radius_mean area_mean smoothness_mean ## Min. : 6.981 Min. : 143.5 Min. :0.05263 ## 1st Qu.:11.700 1st Qu.: 420.3 1st Qu.:0.08637 ## Median :13.370 Median : 551.1 Median :0.09587 ## Mean :14.127 Mean : 654.9 Mean :0.09636 ## 3rd Qu.:15.780 3rd Qu.: 782.7 3rd Qu.:0.10530 ## Max. :28.110 Max. :2501.0 Max. :0.16340 You’ll notice that the three variables have very different ranges and as a consequence area_mean will have a larger impact on the distance calculation than the smootness_mean. This could potentially cause problems for modeling. To solve this we’ll apply normalization to rescale all features to a standard range of values. We will write our own normalization function. normalize &lt;- function(x) { # Function takes in a vector return ((x - min(x)) / (max(x) - min(x))) # distance of item value - minimum vector value divided by the range of all vector values } testSet1 &lt;- c(1:5) testSet2 &lt;- c(1:5) * 10 cat(&quot;testSet1:&quot;, testSet1, &quot;\\n&quot;) ## testSet1: 1 2 3 4 5 cat(&quot;testSet2:&quot;, testSet2, &quot;\\n&quot;) ## testSet2: 10 20 30 40 50 cat(&quot;Normalized testSet1:&quot;, normalize(testSet1), &quot;\\n&quot;) ## Normalized testSet1: 0 0.25 0.5 0.75 1 cat(&quot;Normalized testSet2:&quot;, normalize(testSet2)) ## Normalized testSet2: 0 0.25 0.5 0.75 1 We’ll apply the normalize() function to each feature in the dataset (so, not on the label) using the sapply() function. nCols &lt;- dim(cleanDF)[2] cleanDF_n &lt;- sapply(2:nCols, function(x) { normalize(cleanDF[,x]) }) %&gt;% as.data.frame() summary(cleanDF_n[c(&quot;radius_mean&quot;, &quot;area_mean&quot;, &quot;smoothness_mean&quot;)]) ## radius_mean area_mean smoothness_mean ## Min. :0.0000 Min. :0.0000 Min. :0.0000 ## 1st Qu.:0.2233 1st Qu.:0.1174 1st Qu.:0.3046 ## Median :0.3024 Median :0.1729 Median :0.3904 ## Mean :0.3382 Mean :0.2169 Mean :0.3948 ## 3rd Qu.:0.4164 3rd Qu.:0.2711 3rd Qu.:0.4755 ## Max. :1.0000 Max. :1.0000 Max. :1.0000 When we take the variables we selected earlier and look at the summary parameters again, we’ll see that the normalization was successful. We can now split our data into training and test sets. trainDF_feat &lt;- cleanDF_n[1:469, ] testDF_feat &lt;- cleanDF_n[470:569, ] When creating the training and test sets, we’ve excluded the labels. We’ll create separate training and tests sets for them too. trainDF_labels &lt;- cleanDF[1:469, 1] testDF_labels &lt;- cleanDF[470:569, 1] Now we can train and evaluate our kNN model. 2.4 Modeling To train the knn model we only need one single function from the class package. It takes the set with training features and the set with training label. The trained model is applied to the set with test features and the function gives back a set of predictions. cleanDF_test_pred &lt;- knn(train = as.matrix(trainDF_feat), test = as.matrix(testDF_feat), cl = as.matrix(trainDF_labels), k = 21) %&gt;% as.data.frame() head(cleanDF_test_pred) ## . ## 1 Benign ## 2 Benign ## 3 Benign ## 4 Benign ## 5 Malignant ## 6 Benign 2.5 Evaluation Now that we have a set of predicted labels we can compare these with the actual labels. A diffusion table shows how well the model performed. Figure 2.1: Standard diffusion table. Taken from: https://emj.bmj.com/content/emermed/36/7/431/F1.large.jpg Here is our own table: actual &lt;- testDF_labels[[1]] predicted &lt;- cleanDF_test_pred[[1]] CrossTable(x = predicted, y = actual, prop.chisq = FALSE) ## ## ## Cell Contents ## |-------------------------| ## | N | ## | N / Row Total | ## | N / Col Total | ## | N / Table Total | ## |-------------------------| ## ## ## Total Observations in Table: 100 ## ## ## | actual ## predicted | Benign | Malignant | Row Total | ## -------------|-----------|-----------|-----------| ## Benign | 61 | 2 | 63 | ## | 0.968 | 0.032 | 0.630 | ## | 1.000 | 0.051 | | ## | 0.610 | 0.020 | | ## -------------|-----------|-----------|-----------| ## Malignant | 0 | 37 | 37 | ## | 0.000 | 1.000 | 0.370 | ## | 0.000 | 0.949 | | ## | 0.000 | 0.370 | | ## -------------|-----------|-----------|-----------| ## Column Total | 61 | 39 | 100 | ## | 0.610 | 0.390 | | ## -------------|-----------|-----------|-----------| ## ## We have reached an accuracy of 98%. 2.6 Deployment References "]]
