[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Mining in Python",
    "section": "",
    "text": "Preface\nCRISP-DM Model taken from: https://commons.wikimedia.org/wiki/File:CRISP-DM_Process_Diagram.png\nData mining is the process of sorting through large datasets to identify patterns or relationships to inform business decisions. It is a crucial aspect of modern data analytics, particularly for industries that rely heavily on large amounts of data to inform their business operations.\nWitek ten Hove is a senior instructor and researcher at HAN University of Applied Sciences. His main areas of expertise are Data en Web Technologies.\nThrough his extensive business experience in Finance and International Trade and thorough knowledge of modern data technologies, he is able to make connections between technology and business. As an open source evangelist he firmly believe in the power of knowledge sharing. His mission is to inspire business professionals and help them exploit the full potential of smart technologies.\nHe is the owner of Ten Hove Business Data Solutions, a consultancy and training company helping organizations to achieve maximum business value through data driven solutions."
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "Data Mining in Python",
    "section": "Prerequisites",
    "text": "Prerequisites\nBefore starting this module make sure you have:\n\naccess to the book Nield, T. (2022). Essential Math for Data Science. O’Reilly Media, Inc.\na data science environment setup"
  },
  {
    "objectID": "index.html#purpose-of-this-course",
    "href": "index.html#purpose-of-this-course",
    "title": "Data Mining in Python",
    "section": "Purpose of this course",
    "text": "Purpose of this course\nThe general learning outcome of this course is:\n\nThe student is able to perform a well-defined task independently in a relatively clearly arranged situation, or is able to perform in a complex and unpredictable situation under supervision.\n\nThe course will provide you with a few essential data mining skills. The focus will lie on non-linear modeling techniques - k-Nearest Neighbors (kNN) and Naive Bayes classification.\nAfter a successful completion of the course, a student can demonstrate his or her ability to:\n\nexplore and prepare data for a given non-linear model\ntrain en test a non-linear model\nevaluate the quality of a trained model"
  },
  {
    "objectID": "index.html#structure-of-the-course",
    "href": "index.html#structure-of-the-course",
    "title": "Data Mining in Python",
    "section": "Structure of the course",
    "text": "Structure of the course\n\n\n\nCourse overview\n\n\n\n\n\n\n\nWeek nr.\nModule name\nReadings\n\n\n\n\n2\nOnboarding and Data Exploration\n\n\n\n3-4\nLazy Learning with kNN\nNield Ch.1 up to and including ‘Exponents’\n\n\n5-6\nProbabilistic Learning with Naive Bayes Classification\nNield Ch.2 up to and including ‘Probablity Math’, Ch.3, Ch.4 up to and including ‘What Is a Vector?’\n\n\n7\nProject Application\n\n\n\n\n\n\nThrough the whole of the program you’ll be working on your own data mining projects:\n\nYou will setup your own data science environment\nFind and choose datasets for your projects\nRun several full data mining cycles\nDocument and share your learnings\nDemonstrate you newly acquired competences and skills\n\nMake sure all steps in the data mining process are properly documented. The quality of documentation must be such that an informed data specialist must be able to understand the challenge and the conclusions, the design decisions and the reasons for the choices made during the process.\n\nStretch and Challenge: Advanced students can further research and explore new algorithms for data mining, comparing their performance with KNN and Naive Bayes.\nInclusion: Students who are struggling can work with a partner or teacher during activities to ensure they comprehend the material."
  },
  {
    "objectID": "index.html#essential-math",
    "href": "index.html#essential-math",
    "title": "Data Mining in Python",
    "section": "Essential Math",
    "text": "Essential Math\n\nFor k-Nearest Neighbors\nAn essential element of the k-Nearest Neighbor model is distance. Several methods exist to calculate the distance between two points. One is the Euclidean distance. Let point \\(p\\) have Cartesian coordinates \\((p_1,p_2)\\) and let point \\(q\\) have coordinates \\((q_1,q_2)\\). Then the distance between \\(p\\) and \\(q\\) is given by:\n\\[\nd(p,q) = \\sqrt{\\sum_{i=1}^2{(p_i-q_i)^2}}\n\\]\nFor higher dimensions \\(n\\) this becomes:\n\\[\nd(p,q) = \\sqrt{\\sum_{i=1}^n{(p_i-q_i)^2}}\n\\]\nImportant math topics:\n\nOrder of operation: deduct or square first?\nVariables and types: what are the variables in the above formulas and of what type are they?\nFunctions: which are the dependent and which the independent variables?\nSummations: what is the value of \\(\\sum_{i=3}^4{(i^2)}\\)\nExponents: what is the value of \\((\\sum_{i=3}^4{(i^2)})^{-\\frac{1}{2}}\\)\n\n\n\nFor Naive Bayes\n\nProbability math\nDescriptive statistics\nVectors"
  },
  {
    "objectID": "setup.html#working-with-git-and-github",
    "href": "setup.html#working-with-git-and-github",
    "title": "1  Setting up your data science environment",
    "section": "1.1 Working with Git and Github",
    "text": "1.1 Working with Git and Github"
  },
  {
    "objectID": "setup.html#using-python-virtual-environments",
    "href": "setup.html#using-python-virtual-environments",
    "title": "1  Setting up your data science environment",
    "section": "1.2 Using Python virtual environments",
    "text": "1.2 Using Python virtual environments"
  },
  {
    "objectID": "setup.html#visual-studio-code",
    "href": "setup.html#visual-studio-code",
    "title": "1  Setting up your data science environment",
    "section": "1.3 Visual Studio Code",
    "text": "1.3 Visual Studio Code"
  },
  {
    "objectID": "setup.html#working-with-quarto",
    "href": "setup.html#working-with-quarto",
    "title": "1  Setting up your data science environment",
    "section": "1.4 Working with Quarto",
    "text": "1.4 Working with Quarto"
  },
  {
    "objectID": "understand.html",
    "href": "understand.html",
    "title": "2  Data Understanding",
    "section": "",
    "text": "Links:\n\nwww.kaggle.com/\ndatasetsearch.research.google…\ndata.fivethirtyeight.com/\ndata.gov/\ngithub.com/search?q=dataset\ndata.nasa.gov/\nselected datasets\n\nOnce you have accessed your dataset you’ll want to get familiar with the content and gain insights into its quality and structure. Data analysts or data scientists collect and examine the data to understand its relevance to the project’s goals. They explore the data using various techniques, such as descriptive statistics, data visualization, and data profiling. The goal is to identify patterns, relationships, and potential issues within the dataset, which helps in formulating initial hypotheses and refining the project’s objectives.\n\n\n\nLesson outline\n\n\n\n\n\n\n\n\n\n\n\nTopic\nTasks\nActivities\nStudent\nTeacher\n\n\n\n\n1\nFinding Data\nExplore the different sources of data that may be used in data mining, and how to extract and access this data.\nThink-Pair-Share: students will individually brainstorm potential sources of data, pair up with a partner to discuss, and then share with the class.\n‘We learned about various data sources and perspectives of different students during the brainstorming activity.’\n‘Our objective here is to generate a list of possible sources of data that we can use for data mining. As a teacher, I want you to participate actively in brainstorming and support each other’s thoughts. As students, you will be able to collaborate and gain insights from your peers.’\n\n\n2\nDescriptive Statistics\nCalculate basic descriptive statistics that are commonly used in data mining, and understand how they are used to summarize datasets.\nJigsaw: students will be grouped into teams and tasked to gather data from various sources, conduct descriptive statistics, and report their findings to the rest of the class.\n‘We learned the importance of teamwork, critical thinking, and communication skills by working together to conduct descriptive statistics on our assigned data set.’\n‘The goal here is to give every student a chance to delve deeper into specific aspects of data mining. As a teacher, my role is to facilitate the group and ensure everyone is participating. As students, you are expected to synthesize, analyze, and present your findings through a collaborative effort.’"
  },
  {
    "objectID": "knn.html#business-case-diagnosing-breast-cancer",
    "href": "knn.html#business-case-diagnosing-breast-cancer",
    "title": "3  Lazy Learning with k-Nearest Neighbors",
    "section": "3.1 Business Case: Diagnosing Breast Cancer",
    "text": "3.1 Business Case: Diagnosing Breast Cancer\nBreast cancer is the top cancer in women both in the developed and the developing world. In the Netherlands it is the most pervasive form of cancer (“WHO  Cancer Country Profiles 2020” n.d.). In order to improve breast cancer outcome and survival early detection remains the most important instrument for breast cancer control. If machine learning could automate the identification of cancer, it would improve efficiency of the detection process and might also increase its effectiveness by providing greater detection accuracy."
  },
  {
    "objectID": "knn.html#data-understanding",
    "href": "knn.html#data-understanding",
    "title": "3  Lazy Learning with k-Nearest Neighbors",
    "section": "3.2 Data Understanding",
    "text": "3.2 Data Understanding\nThe data we will be using comes from the University of Wisconsin and is available online as an open source dataset (“UCI Machine Learning Repository: Breast Cancer Wisconsin (Diagnostic) Data Set” n.d.). It includes measurements from digitized images from from fine-needle aspirates of breast mass. The values represent cell nuclei features.\nFor convenience the data in csv format is stored on Github. We can access it directly using a function for reading csv from the pandas library\n\nurl = \"https://raw.githubusercontent.com/businessdatasolutions/courses/main/data%20mining/gitbook/datasets/breastcancer.csv\"\nrawDF = pd.read_csv(url)\n\nUsing the info() function we can have some basic information about the dataset.\n\nrawDF.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 569 entries, 0 to 568\nData columns (total 32 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   id                 569 non-null    int64  \n 1   diagnosis          569 non-null    object \n 2   radius_mean        569 non-null    float64\n 3   texture_mean       569 non-null    float64\n 4   perimeter_mean     569 non-null    float64\n 5   area_mean          569 non-null    float64\n 6   smoothness_mean    569 non-null    float64\n 7   compactness_mean   569 non-null    float64\n 8   concavity_mean     569 non-null    float64\n 9   points_mean        569 non-null    float64\n 10  symmetry_mean      569 non-null    float64\n 11  dimension_mean     569 non-null    float64\n 12  radius_se          569 non-null    float64\n 13  texture_se         569 non-null    float64\n 14  perimeter_se       569 non-null    float64\n 15  area_se            569 non-null    float64\n 16  smoothness_se      569 non-null    float64\n 17  compactness_se     569 non-null    float64\n 18  concavity_se       569 non-null    float64\n 19  points_se          569 non-null    float64\n 20  symmetry_se        569 non-null    float64\n 21  dimension_se       569 non-null    float64\n 22  radius_worst       569 non-null    float64\n 23  texture_worst      569 non-null    float64\n 24  perimeter_worst    569 non-null    float64\n 25  area_worst         569 non-null    float64\n 26  smoothness_worst   569 non-null    float64\n 27  compactness_worst  569 non-null    float64\n 28  concavity_worst    569 non-null    float64\n 29  points_worst       569 non-null    float64\n 30  symmetry_worst     569 non-null    float64\n 31  dimension_worst    569 non-null    float64\ndtypes: float64(30), int64(1), object(1)\nmemory usage: 142.4+ KB"
  },
  {
    "objectID": "knn.html#preparation",
    "href": "knn.html#preparation",
    "title": "3  Lazy Learning with k-Nearest Neighbors",
    "section": "3.3 Preparation",
    "text": "3.3 Preparation\nThe first variable, id, contains unique patient IDs. The IDs do not possess any relevant information for making predictions, so we will delete it from the dataset.\n\ncleanDF = rawDF.drop(['id'], axis=1)\ncleanDF.head()\n\n  diagnosis  radius_mean  ...  symmetry_worst  dimension_worst\n0         B        12.32  ...          0.2827          0.06771\n1         B        10.60  ...          0.2940          0.07587\n2         B        11.04  ...          0.2998          0.07881\n3         B        11.28  ...          0.2102          0.06784\n4         B        15.19  ...          0.2487          0.06766\n\n[5 rows x 31 columns]\n\n\nThe variable named diagnosis contains the outcomes we would like to predict - ‘B’ for ‘Benign’ and ‘M’ for ‘Malignant’. The variable we would like to predict is called the ‘label’. We can look at the counts for both outcomes, using the value_counts() function. When we set the normalize setting to True we get the the proportions.\n\ncntDiag = cleanDF['diagnosis'].value_counts()\npropDiag = cleanDF['diagnosis'].value_counts(normalize=True)\ncntDiag\n\ndiagnosis\nB    357\nM    212\nName: count, dtype: int64\n\npropDiag\n\ndiagnosis\nB    0.627417\nM    0.372583\nName: proportion, dtype: float64\n\n\nLooking again at the results from the info() function you’ll notice that the variable diagnosis is coded as text (object). Many models require that the label is of type category. The pandas library has a function that can transform a object type to category.\n\ncatType = CategoricalDtype(categories=[\"B\", \"M\"], ordered=False)\ncleanDF['diagnosis'] = cleanDF['diagnosis'].astype(catType)\ncleanDF['diagnosis']\n\n0      B\n1      B\n2      B\n3      B\n4      B\n      ..\n564    B\n565    B\n566    M\n567    B\n568    M\nName: diagnosis, Length: 569, dtype: category\nCategories (2, object): ['B', 'M']\n\n\nThe features consist of three different measurements of ten characteristics. We will take three characteristics and have a closer look.\n\ncleanDF[['radius_mean', 'area_mean', 'smoothness_mean']].describe()\n\n       radius_mean    area_mean  smoothness_mean\ncount   569.000000   569.000000       569.000000\nmean     14.127292   654.889104         0.096360\nstd       3.524049   351.914129         0.014064\nmin       6.981000   143.500000         0.052630\n25%      11.700000   420.300000         0.086370\n50%      13.370000   551.100000         0.095870\n75%      15.780000   782.700000         0.105300\nmax      28.110000  2501.000000         0.163400\n\n\nYou’ll notice that the three variables have very different ranges and as a consequence area_mean will have a larger impact on the distance calculation than the smootness_mean. This could potentially cause problems for modeling. To solve this we’ll apply normalization to rescale all features to a standard range of values.\nWe will write our own normalization function,\n\ndef normalize(x):\n  return((x - min(x)) / (max(x) - min(x))) # distance of item value - minimum vector value divided by the range of all vector values\n\ntestSet1 = np.arange(1,6)\ntestSet2 = np.arange(1,6) * 10\n\n\n\nprint(f'testSet1: {testSet1}\\n')\n\ntestSet1: [1 2 3 4 5]\n\nprint(f'testSet2: {testSet2}\\n')\n\ntestSet2: [10 20 30 40 50]\n\nprint(f'Normalized testSet1: {normalize(testSet1)}\\n')\n\nNormalized testSet1: [0.   0.25 0.5  0.75 1.  ]\n\nprint(f'Normalized testSet2: {normalize(testSet2)}\\n')\n\nNormalized testSet2: [0.   0.25 0.5  0.75 1.  ]\n\n\nand apply it to all the numerical variables in the dataframe.\n\nexcluded = ['diagnosis'] # list of columns to exclude\nX = cleanDF.loc[:, ~cleanDF.columns.isin(excluded)]\nX = X.apply(normalize, axis=0)\nX[['radius_mean', 'area_mean', 'smoothness_mean']].describe()\n\n       radius_mean   area_mean  smoothness_mean\ncount   569.000000  569.000000       569.000000\nmean      0.338222    0.216920         0.394785\nstd       0.166787    0.149274         0.126967\nmin       0.000000    0.000000         0.000000\n25%       0.223342    0.117413         0.304595\n50%       0.302381    0.172895         0.390358\n75%       0.416442    0.271135         0.475490\nmax       1.000000    1.000000         1.000000\n\n\nWhen we take the variables we’ve selected earlier and look at the summary parameters again, we’ll see that the normalization was successful.\nWe can now split our data into training and test sets.\n\ny = cleanDF['diagnosis']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123, stratify=y)\n\nHere, X_train and y_train are the features and labels of the training data, respectively, and X_test and y_test are the features and labels of the test data.\nNow we can train and evaluate our kNN model."
  },
  {
    "objectID": "knn.html#modeling-and-evaluation",
    "href": "knn.html#modeling-and-evaluation",
    "title": "3  Lazy Learning with k-Nearest Neighbors",
    "section": "3.4 Modeling and Evaluation",
    "text": "3.4 Modeling and Evaluation\nKNN is a instance-based learning algorithm. It stores all of the training data and makes predictions based on the similarity between the input instance and the stored instances. The prediction is based on the majority class among the K nearest neighbors of the input instance.\nThe distance between instances is typically measured using the Euclidean distance. However, other distance measures such as the Manhattan distance or the Minkowski distance can also be used.\nThe pseudocode for the KNN algorithm is as follows:\n\n\nfor each instance in the test set:\n    for each instance in the training set:\n        calculate the distance between the two instances\n    sort the distances in ascending order\n    find the K nearest neighbors\n    predict the class based on the majority class among the K nearest neighbors\n\n\nTo train the knn model we only need one single function from the sklearn library. The fit() function trains the model on the training data. The trained model is applied to the set with test features and the predict() function gives back a set of predicted values for y.\n\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train, y_train)\n\nKNeighborsClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsClassifierKNeighborsClassifier()\n\n# make predictions on the test set\ny_pred = knn.predict(X_test)\n\nNow that we have a set of predicted labels we can compare these with the actual labels. A diffusion table shows how well the model performed.\n\n\n\n\n\nStandard diffusion table. Taken from: https://emj.bmj.com/content/emermed/36/7/431/F1.large.jpg\n\n\n\n\nHere is our own table:\n\ncm = confusion_matrix(y_test, y_pred, labels=knn.classes_)\ncm\n\narray([[106,   1],\n       [  2,  62]])\n\n\n\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=knn.classes_)\ndisp.plot()\n\n&lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay object at 0x1a2079c10&gt;\n\nplt.show()\n\n\n\n\nQuestions:\n\nHow would you assess the overall performance of the model?\nWhat would you consider as more costly: high false negatives or high false positives levels? Why?\nTry to improve the model by changing some parameters of the KNeighborsClassifier() function\n\n\n\n\n\n“UCI Machine Learning Repository: Breast Cancer Wisconsin (Diagnostic) Data Set.” n.d. Accessed January 7, 2021. https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic).\n\n\n“WHO  Cancer Country Profiles 2020.” n.d. WHO. Accessed January 7, 2021. http://www.who.int/cancer/country-profiles/en/."
  },
  {
    "objectID": "nb.html#business-case-filtering-spam",
    "href": "nb.html#business-case-filtering-spam",
    "title": "4  Probabilistic Learning with Naive Bayes Classification",
    "section": "4.1 Business Case: Filtering Spam",
    "text": "4.1 Business Case: Filtering Spam\nIn 2020 spam accounted for more than 50% of total e-mail traffic (“Spam Statistics: Spam e-Mail Traffic Share 2019” n.d.). This illustrates the value of a good spam filter. Naive Bayes spam filtering is a standard technique for handling spam. It is one of the oldest ways of doing spam filtering, with roots in the 1990s."
  },
  {
    "objectID": "nb.html#data-understanding",
    "href": "nb.html#data-understanding",
    "title": "4  Probabilistic Learning with Naive Bayes Classification",
    "section": "4.2 Data Understanding",
    "text": "4.2 Data Understanding\nThe data you’ll be using comes from the SMS Spam Collection (“UCI Machine Learning Repository: SMS Spam Collection Data Set” n.d.). It contains a set of SMS messages that are labeled ‘ham’ or ‘spam’. and is a standard data set for testing spam filtering methods.\n\nurl = \"https://raw.githubusercontent.com/businessdatasolutions/courses/main/datamining-n/datasets/smsspam.csv\"\nrawDF = pd.read_csv(url)\nrawDF.head()\n\n   type                                               text\n0   ham  Go until jurong point, crazy.. Available only ...\n1   ham                      Ok lar... Joking wif u oni...\n2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n3   ham  U dun say so early hor... U c already then say...\n4   ham  Nah I don't think he goes to usf, he lives aro...\n\n\nThe variable type is of class object which in Python refers to text. As this variable indicates whether the message belongs to the category ham or spam it is better to convert it to a category variable.\n\ncatType = CategoricalDtype(categories=[\"ham\", \"spam\"], ordered=False)\nrawDF.type = rawDF.type.astype(catType)\nrawDF.type\n\n0        ham\n1        ham\n2       spam\n3        ham\n4        ham\n        ... \n5567    spam\n5568     ham\n5569     ham\n5570     ham\n5571     ham\nName: type, Length: 5572, dtype: category\nCategories (2, object): ['ham', 'spam']\n\n\nTo see how the types of sms messages are distributed you can compare the counts for each category.\n\nrawDF.type.value_counts()\n\ntype\nham     4825\nspam     747\nName: count, dtype: int64\n\n\nOften you’ll prefer the relative counts.\n\nrawDF.type.value_counts(normalize=True)\n\ntype\nham     0.865937\nspam    0.134063\nName: proportion, dtype: float64\n\n\nYou can also visually inspect the data by creating wordclouds for each sms type.\n\n# Generate a word cloud image]\nhamText = ' '.join([Text for Text in rawDF[rawDF['type']=='ham']['text']])\nspamText = ' '.join([Text for Text in rawDF[rawDF['type']=='spam']['text']])\ncolorListHam=['#e9f6fb','#92d2ed','#2195c5']\ncolorListSpam=['#f9ebeb','#d57676','#b03636']\ncolormapHam=colors.ListedColormap(colorListHam)\ncolormapSpam=colors.ListedColormap(colorListSpam)\nwordcloudHam = WordCloud(background_color='white', colormap=colormapHam).generate(hamText)\nwordcloudSpam = WordCloud(background_color='white', colormap=colormapSpam).generate(spamText)\n\n# Display the generated image:\n# the matplotlib way:\nfig, (wc1, wc2) = plt.subplots(1, 2)\nfig.suptitle('Wordclouds for ham and spam')\nwc1.imshow(wordcloudHam)\nwc2.imshow(wordcloudSpam)\nplt.show()\n\n\n\n\nQuestion:\n\nWhat differences do you notice?"
  },
  {
    "objectID": "nb.html#preparation",
    "href": "nb.html#preparation",
    "title": "4  Probabilistic Learning with Naive Bayes Classification",
    "section": "4.3 Preparation",
    "text": "4.3 Preparation\nAfter you’ve glimpsed over the data and have a certain understanding of its structure and content, you are now ready to prepare the data for further processing. For the naive bayes model you’ll need to have a dataframe with wordcounts. To save on computation time you can set a limit on the number of features (columns) in the wordsDF dataframe.\n\nvectorizer = TfidfVectorizer(max_features=1000)\nvectors = vectorizer.fit_transform(rawDF.text)\nwordsDF = pd.DataFrame(vectors.toarray(), columns=vectorizer.get_feature_names_out())\nwordsDF.head()\n\n   000   03   04  0800  08000839402  ...  your  yours  yourself   yr  yup\n0  0.0  0.0  0.0   0.0          0.0  ...   0.0    0.0       0.0  0.0  0.0\n1  0.0  0.0  0.0   0.0          0.0  ...   0.0    0.0       0.0  0.0  0.0\n2  0.0  0.0  0.0   0.0          0.0  ...   0.0    0.0       0.0  0.0  0.0\n3  0.0  0.0  0.0   0.0          0.0  ...   0.0    0.0       0.0  0.0  0.0\n4  0.0  0.0  0.0   0.0          0.0  ...   0.0    0.0       0.0  0.0  0.0\n\n[5 rows x 1000 columns]\n\n\nThe counts are normalized in such a way that the words that are most likely to have predictive power get heavier weights. For instance stopword like “a” and “for” most probably will equally likely feature in spam as in ham messages. Therefore these words will be assigned lower normalized counts.\nBefore we start modeling we need to split all datasets into train and test sets. The function train_test_split() can be used to create balanced splits of the data. In this case we’ll create a 75/25% split.\n\nxTrain, xTest, yTrain, yTest = train_test_split(wordsDF, rawDF.type)"
  },
  {
    "objectID": "nb.html#modeling-and-evaluation",
    "href": "nb.html#modeling-and-evaluation",
    "title": "4  Probabilistic Learning with Naive Bayes Classification",
    "section": "4.4 Modeling and Evaluation",
    "text": "4.4 Modeling and Evaluation\nWe have now everything in place to start training our model and evaluate against our test dataset. The MultinomialNB().fit() function is part of the scikit learn package. It takes in the features and labels of our training dataset and returns a trained naive bayes model.\n\nbayes = MultinomialNB()\nbayes.fit(xTrain, yTrain)\n\nMultinomialNB()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.MultinomialNBMultinomialNB()\n\n\nThe model can be applied to the test features using the predict() function which generates a array of predictions. Using a confusion matrix we can analyze the performance of our model.\n\n\n\n\n\nStandard diffusion table. Taken from: https://emj.bmj.com/content/emermed/36/7/431/F1.large.jpg\n\n\n\n\n\nyPred = bayes.predict(xTest)\nyTrue = yTest\n\n\naccuracyScore = accuracy_score(yTrue, yPred)\nprint(f'Accuracy: {accuracyScore}')\n\nAccuracy: 0.9676956209619526\n\nmatrix = confusion_matrix(yTrue, yPred)\nlabelNames = pd.Series(['ham', 'spam'])\npd.DataFrame(matrix,\n     columns='Predicted ' + labelNames,\n     index='Is ' + labelNames)\n\n         Predicted ham  Predicted spam\nIs ham            1182               4\nIs spam             41             166\n\n\nQuestions:\n\nWhat do you think is the role of the alpha parameter in the MultinomialNB() function?\nHow would you assess the overall performance of the model?\nWhat would you consider as more costly: high false negatives or high false positives levels? Why?\n\n\n\n\n\n“Spam Statistics: Spam e-Mail Traffic Share 2019.” n.d. Statista. Accessed January 10, 2021. https://www.statista.com/statistics/420391/spam-email-traffic-share/.\n\n\n“UCI Machine Learning Repository: SMS Spam Collection Data Set.” n.d. Accessed January 9, 2021. https://archive.ics.uci.edu/ml/datasets/sms+spam+collection."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "“Spam Statistics: Spam e-Mail Traffic Share 2019.” n.d.\nStatista. Accessed January 10, 2021. https://www.statista.com/statistics/420391/spam-email-traffic-share/.\n\n\n“UCI Machine Learning\nRepository: Breast Cancer\nWisconsin (Diagnostic) Data\nSet.” n.d. Accessed January 7, 2021. https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic).\n\n\n“UCI Machine Learning\nRepository: SMS Spam\nCollection Data Set.” n.d.\nAccessed January 9, 2021. https://archive.ics.uci.edu/ml/datasets/sms+spam+collection.\n\n\n“WHO  Cancer Country\nProfiles 2020.” n.d. WHO. Accessed January 7, 2021. http://www.who.int/cancer/country-profiles/en/."
  }
]